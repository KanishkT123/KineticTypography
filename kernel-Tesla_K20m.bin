//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23306907
// Driver 390.12
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_35, texmode_independent
.address_size 64

	// .globl	composeRGBPixel
// kernel_HistogramRectAllChannelsReduction$localHist has been demoted
// kernel_HistogramRectOneChannelReduction$localHist has been demoted

.entry composeRGBPixel(
	.param .u64 .ptr .global .align 4 composeRGBPixel_param_0,
	.param .u32 composeRGBPixel_param_1,
	.param .u32 composeRGBPixel_param_2,
	.param .u32 composeRGBPixel_param_3,
	.param .u64 .ptr .global .align 4 composeRGBPixel_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd1, [composeRGBPixel_param_0];
	ld.param.u32 	%r3, [composeRGBPixel_param_1];
	ld.param.u32 	%r5, [composeRGBPixel_param_2];
	ld.param.u32 	%r4, [composeRGBPixel_param_3];
	ld.param.u64 	%rd2, [composeRGBPixel_param_4];
	mov.b32	%r6, %envreg4;
	mov.u32 	%r7, %ctaid.y;
	mov.u32 	%r8, %ntid.y;
	mad.lo.s32 	%r9, %r7, %r8, %r6;
	mov.u32 	%r10, %tid.y;
	add.s32 	%r1, %r9, %r10;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mov.b32	%r13, %envreg3;
	mad.lo.s32 	%r14, %r11, %r12, %r13;
	mov.u32 	%r15, %tid.x;
	add.s32 	%r2, %r14, %r15;
	setp.lt.s32	%p1, %r1, %r5;
	setp.lt.s32	%p2, %r2, %r3;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB0_2;
	bra.uni 	BB0_1;

BB0_1:
	mad.lo.s32 	%r16, %r1, %r3, %r2;
	mul.wide.s32 	%rd3, %r16, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.u32 	%r17, [%rd4];
	shr.u32 	%r18, %r17, 8;
	shl.b32 	%r19, %r17, 8;
	shl.b32 	%r20, %r17, 24;
	and.b32  	%r21, %r19, 16711680;
	or.b32  	%r22, %r21, %r20;
	and.b32  	%r23, %r18, 65280;
	or.b32  	%r24, %r22, %r23;
	mad.lo.s32 	%r25, %r1, %r4, %r2;
	mul.wide.s32 	%rd5, %r25, 4;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.u32 	[%rd6], %r24;

BB0_2:
	ret;
}

	// .globl	pixSubtract_inplace
.entry pixSubtract_inplace(
	.param .u64 .ptr .global .align 4 pixSubtract_inplace_param_0,
	.param .u64 .ptr .global .align 4 pixSubtract_inplace_param_1,
	.param .u32 pixSubtract_inplace_param_2,
	.param .u32 pixSubtract_inplace_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [pixSubtract_inplace_param_0];
	ld.param.u64 	%rd2, [pixSubtract_inplace_param_1];
	ld.param.u32 	%r2, [pixSubtract_inplace_param_2];
	ld.param.u32 	%r3, [pixSubtract_inplace_param_3];
	mov.b32	%r4, %envreg4;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %ntid.y;
	mad.lo.s32 	%r7, %r5, %r6, %r4;
	mov.u32 	%r8, %tid.y;
	add.s32 	%r9, %r7, %r8;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %ntid.x;
	mov.b32	%r12, %envreg3;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r15, %r13, %r14;
	mad.lo.s32 	%r1, %r9, %r2, %r15;
	setp.lt.u32	%p1, %r9, %r3;
	setp.lt.u32	%p2, %r15, %r2;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB1_2;
	bra.uni 	BB1_1;

BB1_1:
	mul.wide.u32 	%rd3, %r1, 4;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.u32 	%r16, [%rd4];
	not.b32 	%r17, %r16;
	add.s64 	%rd5, %rd1, %rd3;
	ld.global.u32 	%r18, [%rd5];
	and.b32  	%r19, %r18, %r17;
	st.global.u32 	[%rd5], %r19;

BB1_2:
	ret;
}

	// .globl	pixSubtract
.entry pixSubtract(
	.param .u64 .ptr .global .align 4 pixSubtract_param_0,
	.param .u64 .ptr .global .align 4 pixSubtract_param_1,
	.param .u32 pixSubtract_param_2,
	.param .u32 pixSubtract_param_3,
	.param .u64 .ptr .global .align 4 pixSubtract_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd1, [pixSubtract_param_0];
	ld.param.u64 	%rd2, [pixSubtract_param_1];
	ld.param.u32 	%r2, [pixSubtract_param_2];
	ld.param.u32 	%r3, [pixSubtract_param_3];
	ld.param.u64 	%rd3, [pixSubtract_param_4];
	mov.b32	%r4, %envreg4;
	mov.u32 	%r5, %ctaid.y;
	mov.u32 	%r6, %ntid.y;
	mad.lo.s32 	%r7, %r5, %r6, %r4;
	mov.u32 	%r8, %tid.y;
	add.s32 	%r9, %r7, %r8;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %ntid.x;
	mov.b32	%r12, %envreg3;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r15, %r13, %r14;
	mad.lo.s32 	%r1, %r9, %r2, %r15;
	setp.lt.u32	%p1, %r9, %r3;
	setp.lt.u32	%p2, %r15, %r2;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB2_2;
	bra.uni 	BB2_1;

BB2_1:
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd5, %rd1, %rd4;
	add.s64 	%rd6, %rd2, %rd4;
	ld.global.u32 	%r16, [%rd6];
	not.b32 	%r17, %r16;
	ld.global.u32 	%r18, [%rd5];
	and.b32  	%r19, %r18, %r17;
	add.s64 	%rd7, %rd3, %rd4;
	st.global.u32 	[%rd7], %r19;

BB2_2:
	ret;
}

	// .globl	morphoDilateHor_5x5
.entry morphoDilateHor_5x5(
	.param .u64 .ptr .global .align 4 morphoDilateHor_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateHor_5x5_param_1,
	.param .u32 morphoDilateHor_5x5_param_2,
	.param .u32 morphoDilateHor_5x5_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoDilateHor_5x5_param_0];
	ld.param.u64 	%rd4, [morphoDilateHor_5x5_param_1];
	ld.param.u32 	%r8, [morphoDilateHor_5x5_param_2];
	ld.param.u32 	%r9, [morphoDilateHor_5x5_param_3];
	mov.b32	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r13, %r11, %r12, %r10;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r1, %r13, %r14;
	rem.u32 	%r2, %r1, %r8;
	mul.lo.s32 	%r15, %r9, %r8;
	setp.ge.u32	%p1, %r1, %r15;
	@%p1 bra 	BB3_6;

	cvt.u64.u32	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32	%p2, %r2, 0;
	mov.u32 	%r36, 0;
	mov.u32 	%r35, %r36;
	@%p2 bra 	BB3_3;

	ld.global.u32 	%r35, [%rd2+-4];

BB3_3:
	add.s32 	%r18, %r8, -1;
	setp.eq.s32	%p3, %r2, %r18;
	@%p3 bra 	BB3_5;

	ld.global.u32 	%r36, [%rd2+4];

BB3_5:
	shr.u32 	%r19, %r3, 1;
	or.b32  	%r20, %r19, %r3;
	shl.b32 	%r21, %r3, 1;
	or.b32  	%r22, %r20, %r21;
	shr.u32 	%r23, %r3, 2;
	or.b32  	%r24, %r22, %r23;
	shl.b32 	%r25, %r3, 2;
	or.b32  	%r26, %r24, %r25;
	shl.b32 	%r27, %r35, 31;
	or.b32  	%r28, %r26, %r27;
	shl.b32 	%r29, %r35, 30;
	or.b32  	%r30, %r28, %r29;
	shr.u32 	%r31, %r36, 31;
	or.b32  	%r32, %r30, %r31;
	shr.u32 	%r33, %r36, 30;
	or.b32  	%r34, %r32, %r33;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r34;

BB3_6:
	ret;
}

	// .globl	morphoDilateVer_5x5
.entry morphoDilateVer_5x5(
	.param .u64 .ptr .global .align 4 morphoDilateVer_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateVer_5x5_param_1,
	.param .u32 morphoDilateVer_5x5_param_2,
	.param .u32 morphoDilateVer_5x5_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd1, [morphoDilateVer_5x5_param_0];
	ld.param.u64 	%rd2, [morphoDilateVer_5x5_param_1];
	ld.param.u32 	%r3, [morphoDilateVer_5x5_param_2];
	ld.param.u32 	%r4, [morphoDilateVer_5x5_param_3];
	mov.b32	%r5, %envreg3;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %ntid.x;
	mad.lo.s32 	%r8, %r6, %r7, %r5;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r1, %r8, %r9;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r11, %ntid.y;
	mov.b32	%r12, %envreg4;
	mad.lo.s32 	%r13, %r10, %r11, %r12;
	mov.u32 	%r14, %tid.y;
	add.s32 	%r2, %r13, %r14;
	setp.lt.s32	%p1, %r2, %r4;
	setp.lt.s32	%p2, %r1, %r3;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB4_2;
	bra.uni 	BB4_1;

BB4_1:
	cvt.s64.s32	%rd3, %r1;
	mad.lo.s32 	%r15, %r2, %r3, %r1;
	mul.wide.u32 	%rd4, %r15, 4;
	add.s64 	%rd5, %rd1, %rd4;
	add.s32 	%r16, %r2, -2;
	setp.lt.s32	%p4, %r16, 0;
	selp.b32	%r17, %r2, %r16, %p4;
	mul.lo.s32 	%r18, %r17, %r3;
	cvt.s64.s32	%rd6, %r18;
	add.s64 	%rd7, %rd6, %rd3;
	shl.b64 	%rd8, %rd7, 2;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.u32 	%r19, [%rd9];
	ld.global.u32 	%r20, [%rd5];
	or.b32  	%r21, %r19, %r20;
	add.s32 	%r22, %r2, -1;
	setp.lt.s32	%p5, %r22, 0;
	selp.b32	%r23, %r2, %r22, %p5;
	mul.lo.s32 	%r24, %r23, %r3;
	cvt.s64.s32	%rd10, %r24;
	add.s64 	%rd11, %rd10, %rd3;
	shl.b64 	%rd12, %rd11, 2;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.u32 	%r25, [%rd13];
	or.b32  	%r26, %r21, %r25;
	add.s32 	%r27, %r4, -1;
	setp.lt.s32	%p6, %r2, %r27;
	selp.u32	%r28, 1, 0, %p6;
	add.s32 	%r29, %r28, %r2;
	mul.lo.s32 	%r30, %r29, %r3;
	cvt.s64.s32	%rd14, %r30;
	add.s64 	%rd15, %rd14, %rd3;
	shl.b64 	%rd16, %rd15, 2;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.u32 	%r31, [%rd17];
	or.b32  	%r32, %r26, %r31;
	add.s32 	%r33, %r4, -2;
	setp.lt.s32	%p7, %r2, %r33;
	add.s32 	%r34, %r2, 2;
	selp.b32	%r35, %r34, %r2, %p7;
	mul.lo.s32 	%r36, %r35, %r3;
	cvt.s64.s32	%rd18, %r36;
	add.s64 	%rd19, %rd18, %rd3;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u32 	%r37, [%rd21];
	or.b32  	%r38, %r32, %r37;
	add.s64 	%rd22, %rd2, %rd4;
	st.global.u32 	[%rd22], %r38;

BB4_2:
	ret;
}

	// .globl	morphoDilateHor
.entry morphoDilateHor(
	.param .u64 .ptr .global .align 4 morphoDilateHor_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateHor_param_1,
	.param .u32 morphoDilateHor_param_2,
	.param .u32 morphoDilateHor_param_3,
	.param .u32 morphoDilateHor_param_4,
	.param .u32 morphoDilateHor_param_5
)
{
	.reg .pred 	%p<55>;
	.reg .b32 	%r<269>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd8, [morphoDilateHor_param_0];
	ld.param.u64 	%rd9, [morphoDilateHor_param_1];
	ld.param.u32 	%r60, [morphoDilateHor_param_2];
	ld.param.u32 	%r61, [morphoDilateHor_param_3];
	ld.param.u32 	%r62, [morphoDilateHor_param_4];
	ld.param.u32 	%r63, [morphoDilateHor_param_5];
	mov.b32	%r64, %envreg3;
	mov.u32 	%r65, %ctaid.x;
	mov.u32 	%r66, %ntid.x;
	mad.lo.s32 	%r67, %r65, %r66, %r64;
	mov.u32 	%r68, %tid.x;
	add.s32 	%r1, %r67, %r68;
	mov.u32 	%r69, %ctaid.y;
	mov.u32 	%r70, %ntid.y;
	mov.b32	%r71, %envreg4;
	mad.lo.s32 	%r72, %r69, %r70, %r71;
	mov.u32 	%r73, %tid.y;
	add.s32 	%r74, %r72, %r73;
	mul.lo.s32 	%r2, %r74, %r62;
	add.s32 	%r3, %r2, %r1;
	mul.lo.s32 	%r75, %r63, %r62;
	setp.ge.u32	%p2, %r3, %r75;
	@%p2 bra 	BB5_37;

	setp.lt.s32	%p3, %r61, 1;
	setp.lt.s32	%p4, %r60, 1;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	BB5_37;

	cvt.u64.u32	%rd1, %r3;
	mul.wide.u32 	%rd10, %r3, 4;
	add.s64 	%rd2, %rd8, %rd10;
	ld.global.u32 	%r4, [%rd2];
	and.b32  	%r5, %r60, 31;
	setp.eq.s32	%p1, %r5, 0;
	setp.ne.s32	%p6, %r5, 0;
	selp.u32	%r77, 1, 0, %p6;
	shr.s32 	%r78, %r60, 5;
	add.s32 	%r79, %r77, %r78;
	sub.s32 	%r6, %r1, %r79;
	add.s32 	%r7, %r1, %r79;
	setp.eq.s32	%p7, %r1, 0;
	mov.u32 	%r255, 0;
	mov.u32 	%r244, %r255;
	@%p7 bra 	BB5_4;

	ld.global.u32 	%r244, [%rd2+-4];

BB5_4:
	add.s32 	%r81, %r62, -1;
	setp.eq.s32	%p8, %r1, %r81;
	mov.u32 	%r245, %r255;
	@%p8 bra 	BB5_6;

	ld.global.u32 	%r245, [%rd2+4];

BB5_6:
	selp.b32	%r87, 31, %r5, %p1;
	add.s32 	%r88, %r87, 1;
	setp.gt.u32	%p9, %r88, 2;
	selp.b32	%r89, %r87, 1, %p9;
	and.b32  	%r86, %r89, 3;
	mov.u32 	%r246, 1;
	setp.eq.s32	%p10, %r86, 0;
	@%p10 bra 	BB5_7;

	setp.eq.s32	%p11, %r86, 1;
	@%p11 bra 	BB5_9;
	bra.uni 	BB5_10;

BB5_9:
	mov.u32 	%r249, %r4;
	bra.uni 	BB5_13;

BB5_7:
	mov.u32 	%r252, %r4;
	bra.uni 	BB5_14;

BB5_10:
	setp.eq.s32	%p12, %r86, 2;
	mov.u32 	%r247, %r4;
	@%p12 bra 	BB5_12;

	and.b32  	%r93, %r61, 31;
	setp.eq.s32	%p14, %r87, %r93;
	setp.ne.s32	%p15, %r87, 1;
	or.pred  	%p16, %p15, %p14;
	shr.u32 	%r94, %r4, 1;
	shl.b32 	%r95, %r244, 31;
	or.b32  	%r96, %r95, %r94;
	selp.b32	%r97, %r96, 0, %p16;
	shr.u32 	%r98, %r245, 31;
	or.b32  	%r99, %r98, %r4;
	shl.b32 	%r100, %r4, 1;
	or.b32  	%r101, %r99, %r100;
	or.b32  	%r247, %r101, %r97;
	mov.u32 	%r246, 2;

BB5_12:
	and.b32  	%r104, %r61, 31;
	setp.eq.s32	%p18, %r87, %r104;
	setp.ne.s32	%p19, %r246, %r87;
	or.pred  	%p20, %p19, %p18;
	neg.s32 	%r105, %r246;
	and.b32  	%r106, %r105, 31;
	shl.b32 	%r107, %r244, %r106;
	shr.u32 	%r108, %r4, %r246;
	or.b32  	%r109, %r107, %r108;
	selp.b32	%r110, %r109, 0, %p20;
	shr.u32 	%r111, %r245, %r106;
	or.b32  	%r112, %r111, %r247;
	shl.b32 	%r113, %r4, %r246;
	or.b32  	%r114, %r112, %r113;
	or.b32  	%r249, %r114, %r110;
	add.s32 	%r246, %r246, 1;

BB5_13:
	and.b32  	%r117, %r61, 31;
	setp.eq.s32	%p22, %r87, %r117;
	setp.ne.s32	%p23, %r246, %r87;
	or.pred  	%p24, %p23, %p22;
	neg.s32 	%r118, %r246;
	and.b32  	%r119, %r118, 31;
	shl.b32 	%r120, %r244, %r119;
	and.b32  	%r121, %r246, 31;
	shr.u32 	%r122, %r4, %r121;
	or.b32  	%r123, %r120, %r122;
	selp.b32	%r124, %r123, 0, %p24;
	shl.b32 	%r125, %r4, %r121;
	shr.u32 	%r126, %r245, %r119;
	or.b32  	%r127, %r126, %r249;
	or.b32  	%r128, %r127, %r125;
	or.b32  	%r255, %r128, %r124;
	add.s32 	%r246, %r246, 1;
	mov.u32 	%r252, %r255;

BB5_14:
	setp.lt.u32	%p27, %r89, 4;
	and.b32  	%r25, %r61, 31;
	@%p27 bra 	BB5_17;

	mov.u32 	%r255, %r252;

BB5_16:
	setp.ne.s32	%p28, %r246, %r87;
	setp.eq.s32	%p29, %r87, %r25;
	or.pred  	%p30, %p28, %p29;
	neg.s32 	%r132, %r246;
	and.b32  	%r133, %r132, 31;
	shl.b32 	%r134, %r244, %r133;
	and.b32  	%r135, %r246, 31;
	shr.u32 	%r136, %r4, %r135;
	or.b32  	%r137, %r134, %r136;
	selp.b32	%r138, %r137, 0, %p30;
	shl.b32 	%r139, %r4, %r135;
	shr.u32 	%r140, %r245, %r133;
	or.b32  	%r141, %r140, %r255;
	or.b32  	%r142, %r141, %r139;
	or.b32  	%r143, %r142, %r138;
	add.s32 	%r144, %r246, 1;
	setp.ne.s32	%p31, %r144, %r87;
	or.pred  	%p32, %p31, %p29;
	not.b32 	%r145, %r246;
	and.b32  	%r146, %r145, 31;
	and.b32  	%r147, %r144, 31;
	shl.b32 	%r148, %r244, %r146;
	shr.u32 	%r149, %r4, %r147;
	or.b32  	%r150, %r148, %r149;
	selp.b32	%r151, %r150, 0, %p32;
	shl.b32 	%r152, %r4, %r147;
	shr.u32 	%r153, %r245, %r146;
	or.b32  	%r154, %r153, %r143;
	or.b32  	%r155, %r154, %r152;
	or.b32  	%r156, %r155, %r151;
	add.s32 	%r157, %r246, 2;
	setp.ne.s32	%p33, %r157, %r87;
	or.pred  	%p34, %p33, %p29;
	mov.u32 	%r158, -2;
	sub.s32 	%r159, %r158, %r246;
	and.b32  	%r160, %r159, 31;
	and.b32  	%r161, %r157, 31;
	shl.b32 	%r162, %r244, %r160;
	shr.u32 	%r163, %r4, %r161;
	or.b32  	%r164, %r162, %r163;
	selp.b32	%r165, %r164, 0, %p34;
	shl.b32 	%r166, %r4, %r161;
	shr.u32 	%r167, %r245, %r160;
	or.b32  	%r168, %r167, %r156;
	or.b32  	%r169, %r168, %r166;
	or.b32  	%r170, %r169, %r165;
	add.s32 	%r171, %r246, 3;
	setp.ne.s32	%p35, %r171, %r87;
	or.pred  	%p36, %p35, %p29;
	mov.u32 	%r172, -3;
	sub.s32 	%r173, %r172, %r246;
	and.b32  	%r174, %r173, 31;
	and.b32  	%r175, %r171, 31;
	shl.b32 	%r176, %r244, %r174;
	shr.u32 	%r177, %r4, %r175;
	or.b32  	%r178, %r176, %r177;
	selp.b32	%r179, %r178, 0, %p36;
	shl.b32 	%r180, %r4, %r175;
	shr.u32 	%r181, %r245, %r174;
	or.b32  	%r182, %r181, %r170;
	or.b32  	%r183, %r182, %r180;
	or.b32  	%r255, %r183, %r179;
	add.s32 	%r246, %r246, 4;
	setp.le.u32	%p37, %r246, %r87;
	@%p37 bra 	BB5_16;

BB5_17:
	setp.eq.s32	%p39, %r79, 1;
	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd3, %rd9, %rd11;
	@%p39 bra 	BB5_36;
	bra.uni 	BB5_18;

BB5_36:
	setp.eq.s32	%p53, %r61, 32;
	selp.b32	%r240, %r244, 0, %p53;
	setp.eq.s32	%p54, %r60, 32;
	selp.b32	%r241, %r245, 0, %p54;
	or.b32  	%r242, %r241, %r240;
	or.b32  	%r243, %r242, %r255;
	st.global.u32 	[%rd3], %r243;
	bra.uni 	BB5_37;

BB5_18:
	setp.lt.s32	%p40, %r6, 0;
	mov.u32 	%r260, 0;
	mov.u32 	%r259, %r260;
	@%p40 bra 	BB5_20;

	cvt.s64.s32	%rd12, %r2;
	cvt.s64.s32	%rd13, %r6;
	add.s64 	%rd14, %rd12, %rd13;
	shl.b64 	%rd15, %rd14, 2;
	add.s64 	%rd16, %rd8, %rd15;
	ld.global.u32 	%r259, [%rd16];

BB5_20:
	setp.ge.s32	%p41, %r7, %r62;
	@%p41 bra 	BB5_22;

	cvt.s64.s32	%rd17, %r2;
	cvt.s64.s32	%rd18, %r7;
	add.s64 	%rd19, %rd17, %rd18;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd8, %rd20;
	ld.global.u32 	%r260, [%rd21];

BB5_22:
	setp.lt.u32	%p43, %r79, 2;
	@%p43 bra 	BB5_35;

	cvt.s64.s32	%rd22, %r7;
	cvt.s64.s32	%rd23, %r2;
	add.s64 	%rd24, %rd23, %rd22;
	add.s64 	%rd4, %rd24, -1;
	cvt.s64.s32	%rd25, %r6;
	add.s64 	%rd5, %rd23, %rd25;
	mov.u32 	%r258, 1;

BB5_24:
	add.s32 	%r39, %r258, %r6;
	setp.lt.s32	%p44, %r39, 0;
	cvt.s64.s32	%rd26, %r258;
	add.s64 	%rd27, %rd5, %rd26;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd6, %rd8, %rd28;
	mov.u32 	%r266, 0;
	mov.u32 	%r262, %r266;
	@%p44 bra 	BB5_26;

	ld.global.u32 	%r262, [%rd6];

BB5_26:
	neg.s32 	%r197, %r61;
	and.b32  	%r198, %r197, 31;
	shl.b32 	%r199, %r259, %r198;
	shr.u32 	%r201, %r262, %r25;
	or.b32  	%r42, %r201, %r199;
	add.s32 	%r202, %r39, 1;
	setp.lt.s32	%p45, %r202, 0;
	mov.u32 	%r259, %r266;
	@%p45 bra 	BB5_28;

	ld.global.u32 	%r259, [%rd6+4];

BB5_28:
	shl.b32 	%r206, %r262, %r198;
	shr.u32 	%r208, %r259, %r25;
	or.b32  	%r45, %r208, %r206;
	sub.s32 	%r46, %r7, %r258;
	sub.s64 	%rd30, %rd4, %rd26;
	shl.b64 	%rd31, %rd30, 2;
	add.s64 	%rd7, %rd8, %rd31;
	setp.ge.s32	%p46, %r46, %r62;
	mov.u32 	%r264, %r266;
	@%p46 bra 	BB5_30;

	ld.global.u32 	%r264, [%rd7+4];

BB5_30:
	shl.b32 	%r212, %r264, %r87;
	neg.s32 	%r213, %r87;
	and.b32  	%r214, %r213, 31;
	shr.u32 	%r215, %r260, %r214;
	or.b32  	%r49, %r212, %r215;
	add.s32 	%r216, %r46, -1;
	setp.ge.s32	%p48, %r216, %r62;
	mov.u32 	%r260, %r266;
	@%p48 bra 	BB5_32;

	ld.global.u32 	%r260, [%rd7];

BB5_32:
	shl.b32 	%r220, %r260, %r87;
	shr.u32 	%r223, %r264, %r214;
	or.b32  	%r52, %r220, %r223;

BB5_33:
	add.s32 	%r56, %r266, 1;
	shl.b32 	%r224, %r42, %r56;
	mov.u32 	%r225, 31;
	sub.s32 	%r226, %r225, %r266;
	shr.u32 	%r227, %r45, %r226;
	shl.b32 	%r228, %r52, %r56;
	shr.u32 	%r229, %r49, %r226;
	or.b32  	%r230, %r227, %r255;
	or.b32  	%r231, %r230, %r224;
	or.b32  	%r232, %r231, %r229;
	or.b32  	%r255, %r232, %r228;
	setp.ne.s32	%p50, %r56, 31;
	mov.u32 	%r266, %r56;
	@%p50 bra 	BB5_33;

	or.b32  	%r233, %r45, %r42;
	or.b32  	%r234, %r233, %r49;
	or.b32  	%r235, %r234, %r52;
	or.b32  	%r255, %r235, %r255;
	add.s32 	%r258, %r258, 1;
	setp.lt.u32	%p52, %r258, %r79;
	@%p52 bra 	BB5_24;

BB5_35:
	st.global.u32 	[%rd3], %r255;

BB5_37:
	ret;
}

	// .globl	morphoDilateHor_32word
.entry morphoDilateHor_32word(
	.param .u64 .ptr .global .align 4 morphoDilateHor_32word_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateHor_32word_param_1,
	.param .u32 morphoDilateHor_32word_param_2,
	.param .u32 morphoDilateHor_32word_param_3,
	.param .u32 morphoDilateHor_32word_param_4,
	.param .u8 morphoDilateHor_32word_param_5
)
{
	.reg .pred 	%p<28>;
	.reg .b16 	%rs<6>;
	.reg .b32 	%r<142>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoDilateHor_32word_param_0];
	ld.param.u64 	%rd4, [morphoDilateHor_32word_param_1];
	ld.param.u32 	%r25, [morphoDilateHor_32word_param_2];
	ld.param.u32 	%r26, [morphoDilateHor_32word_param_3];
	ld.param.u32 	%r27, [morphoDilateHor_32word_param_4];
	ld.param.s8 	%rs1, [morphoDilateHor_32word_param_5];
	mov.b32	%r28, %envreg3;
	mov.u32 	%r29, %ctaid.x;
	mov.u32 	%r30, %ntid.x;
	mad.lo.s32 	%r31, %r29, %r30, %r28;
	mov.u32 	%r32, %tid.x;
	add.s32 	%r1, %r31, %r32;
	mov.u32 	%r33, %ctaid.y;
	mov.u32 	%r34, %ntid.y;
	mov.b32	%r35, %envreg4;
	mad.lo.s32 	%r36, %r33, %r34, %r35;
	mov.u32 	%r37, %tid.y;
	add.s32 	%r38, %r36, %r37;
	mad.lo.s32 	%r2, %r38, %r26, %r1;
	mul.lo.s32 	%r39, %r27, %r26;
	setp.ge.u32	%p1, %r2, %r39;
	@%p1 bra 	BB6_19;

	cvt.u64.u32	%rd1, %r2;
	mul.wide.u32 	%rd5, %r2, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32	%p2, %r1, 0;
	mov.u32 	%r131, 0;
	mov.u32 	%r130, %r131;
	@%p2 bra 	BB6_3;

	ld.global.u32 	%r130, [%rd2+-4];

BB6_3:
	add.s32 	%r42, %r26, -1;
	setp.eq.s32	%p3, %r1, %r42;
	@%p3 bra 	BB6_5;

	ld.global.u32 	%r131, [%rd2+4];

BB6_5:
	setp.lt.s32	%p4, %r25, 1;
	@%p4 bra 	BB6_6;

	and.b32  	%r47, %r25, 3;
	mov.u32 	%r132, 1;
	mov.u32 	%r141, 0;
	setp.eq.s32	%p5, %r47, 0;
	@%p5 bra 	BB6_8;

	setp.eq.s32	%p6, %r47, 1;
	@%p6 bra 	BB6_10;
	bra.uni 	BB6_11;

BB6_10:
	mov.u32 	%r135, %r3;
	bra.uni 	BB6_14;

BB6_6:
	mov.u32 	%r141, %r3;
	bra.uni 	BB6_18;

BB6_8:
	mov.u32 	%r137, %r3;
	bra.uni 	BB6_15;

BB6_11:
	setp.eq.s32	%p7, %r47, 2;
	mov.u32 	%r133, %r3;
	@%p7 bra 	BB6_13;

	and.b16  	%rs2, %rs1, 255;
	setp.eq.s16	%p8, %rs2, 0;
	setp.ne.s32	%p9, %r25, 1;
	or.pred  	%p10, %p9, %p8;
	shr.u32 	%r49, %r3, 1;
	shl.b32 	%r50, %r130, 31;
	or.b32  	%r51, %r50, %r49;
	selp.b32	%r52, %r51, 0, %p10;
	shr.u32 	%r53, %r131, 31;
	or.b32  	%r54, %r53, %r3;
	shl.b32 	%r55, %r3, 1;
	or.b32  	%r56, %r54, %r55;
	or.b32  	%r133, %r56, %r52;
	mov.u32 	%r132, 2;

BB6_13:
	setp.ne.s32	%p11, %r132, %r25;
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p12, %rs3, 0;
	or.pred  	%p13, %p11, %p12;
	neg.s32 	%r57, %r132;
	and.b32  	%r58, %r57, 31;
	shl.b32 	%r59, %r130, %r58;
	shr.u32 	%r60, %r3, %r132;
	or.b32  	%r61, %r59, %r60;
	selp.b32	%r62, %r61, 0, %p13;
	shr.u32 	%r63, %r131, %r58;
	or.b32  	%r64, %r63, %r133;
	shl.b32 	%r65, %r3, %r132;
	or.b32  	%r66, %r64, %r65;
	or.b32  	%r135, %r66, %r62;
	add.s32 	%r132, %r132, 1;

BB6_14:
	setp.ne.s32	%p14, %r132, %r25;
	and.b16  	%rs4, %rs1, 255;
	setp.eq.s16	%p15, %rs4, 0;
	or.pred  	%p16, %p14, %p15;
	neg.s32 	%r67, %r132;
	and.b32  	%r68, %r67, 31;
	shl.b32 	%r69, %r130, %r68;
	and.b32  	%r70, %r132, 31;
	shr.u32 	%r71, %r3, %r70;
	or.b32  	%r72, %r69, %r71;
	selp.b32	%r73, %r72, 0, %p16;
	shl.b32 	%r74, %r3, %r70;
	shr.u32 	%r75, %r131, %r68;
	or.b32  	%r76, %r75, %r135;
	or.b32  	%r77, %r76, %r74;
	or.b32  	%r137, %r77, %r73;
	add.s32 	%r132, %r132, 1;
	mov.u32 	%r141, %r137;

BB6_15:
	setp.lt.u32	%p17, %r25, 4;
	@%p17 bra 	BB6_18;

	mov.u32 	%r141, %r137;

BB6_17:
	setp.ne.s32	%p18, %r132, %r25;
	and.b16  	%rs5, %rs1, 255;
	setp.eq.s16	%p19, %rs5, 0;
	or.pred  	%p20, %p18, %p19;
	neg.s32 	%r78, %r132;
	and.b32  	%r79, %r78, 31;
	shl.b32 	%r80, %r130, %r79;
	and.b32  	%r81, %r132, 31;
	shr.u32 	%r82, %r3, %r81;
	or.b32  	%r83, %r80, %r82;
	selp.b32	%r84, %r83, 0, %p20;
	shl.b32 	%r85, %r3, %r81;
	shr.u32 	%r86, %r131, %r79;
	or.b32  	%r87, %r86, %r141;
	or.b32  	%r88, %r87, %r85;
	or.b32  	%r89, %r88, %r84;
	add.s32 	%r90, %r132, 1;
	setp.ne.s32	%p21, %r90, %r25;
	or.pred  	%p22, %p21, %p19;
	not.b32 	%r91, %r132;
	and.b32  	%r92, %r91, 31;
	and.b32  	%r93, %r90, 31;
	shl.b32 	%r94, %r130, %r92;
	shr.u32 	%r95, %r3, %r93;
	or.b32  	%r96, %r94, %r95;
	selp.b32	%r97, %r96, 0, %p22;
	shl.b32 	%r98, %r3, %r93;
	shr.u32 	%r99, %r131, %r92;
	or.b32  	%r100, %r99, %r89;
	or.b32  	%r101, %r100, %r98;
	or.b32  	%r102, %r101, %r97;
	add.s32 	%r103, %r132, 2;
	setp.ne.s32	%p23, %r103, %r25;
	or.pred  	%p24, %p23, %p19;
	mov.u32 	%r104, -2;
	sub.s32 	%r105, %r104, %r132;
	and.b32  	%r106, %r105, 31;
	and.b32  	%r107, %r103, 31;
	shl.b32 	%r108, %r130, %r106;
	shr.u32 	%r109, %r3, %r107;
	or.b32  	%r110, %r108, %r109;
	selp.b32	%r111, %r110, 0, %p24;
	shl.b32 	%r112, %r3, %r107;
	shr.u32 	%r113, %r131, %r106;
	or.b32  	%r114, %r113, %r102;
	or.b32  	%r115, %r114, %r112;
	or.b32  	%r116, %r115, %r111;
	add.s32 	%r117, %r132, 3;
	setp.ne.s32	%p25, %r117, %r25;
	or.pred  	%p26, %p25, %p19;
	mov.u32 	%r118, -3;
	sub.s32 	%r119, %r118, %r132;
	and.b32  	%r120, %r119, 31;
	and.b32  	%r121, %r117, 31;
	shl.b32 	%r122, %r130, %r120;
	shr.u32 	%r123, %r3, %r121;
	or.b32  	%r124, %r122, %r123;
	selp.b32	%r125, %r124, 0, %p26;
	shl.b32 	%r126, %r3, %r121;
	shr.u32 	%r127, %r131, %r120;
	or.b32  	%r128, %r127, %r116;
	or.b32  	%r129, %r128, %r126;
	or.b32  	%r141, %r129, %r125;
	add.s32 	%r132, %r132, 4;
	setp.lt.s32	%p27, %r117, %r25;
	@%p27 bra 	BB6_17;

BB6_18:
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r141;

BB6_19:
	ret;
}

	// .globl	morphoDilateVer
.entry morphoDilateVer(
	.param .u64 .ptr .global .align 4 morphoDilateVer_param_0,
	.param .u64 .ptr .global .align 4 morphoDilateVer_param_1,
	.param .u32 morphoDilateVer_param_2,
	.param .u32 morphoDilateVer_param_3,
	.param .u32 morphoDilateVer_param_4,
	.param .u32 morphoDilateVer_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<80>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd4, [morphoDilateVer_param_0];
	ld.param.u64 	%rd5, [morphoDilateVer_param_1];
	ld.param.u32 	%r33, [morphoDilateVer_param_2];
	ld.param.u32 	%r34, [morphoDilateVer_param_3];
	ld.param.u32 	%r35, [morphoDilateVer_param_4];
	ld.param.u32 	%r36, [morphoDilateVer_param_5];
	mov.b32	%r37, %envreg3;
	mov.u32 	%r38, %ctaid.x;
	mov.u32 	%r39, %ntid.x;
	mad.lo.s32 	%r40, %r38, %r39, %r37;
	mov.u32 	%r41, %tid.x;
	add.s32 	%r1, %r40, %r41;
	mov.u32 	%r42, %ctaid.y;
	mov.u32 	%r43, %ntid.y;
	mov.b32	%r44, %envreg4;
	mad.lo.s32 	%r2, %r42, %r43, %r44;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.lt.s32	%p1, %r4, %r35;
	setp.lt.s32	%p2, %r1, %r34;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB7_13;
	bra.uni 	BB7_1;

BB7_1:
	mad.lo.s32 	%r45, %r4, %r34, %r1;
	cvt.u64.u32	%rd1, %r45;
	mul.wide.u32 	%rd6, %r45, 4;
	add.s64 	%rd7, %rd4, %rd6;
	ld.global.u32 	%r5, [%rd7];
	sub.s32 	%r46, %r4, %r36;
	mov.u32 	%r79, 0;
	max.s32 	%r6, %r79, %r46;
	sub.s32 	%r48, %r35, %r33;
	setp.lt.s32	%p4, %r4, %r48;
	add.s32 	%r49, %r4, %r33;
	add.s32 	%r50, %r35, -1;
	selp.b32	%r7, %r49, %r50, %p4;
	setp.gt.s32	%p5, %r6, %r7;
	@%p5 bra 	BB7_2;
	bra.uni 	BB7_3;

BB7_2:
	mov.u32 	%r79, %r5;
	bra.uni 	BB7_12;

BB7_3:
	cvt.s64.s32	%rd2, %r1;
	max.s32 	%r54, %r46, %r79;
	add.s32 	%r55, %r7, 1;
	sub.s32 	%r8, %r55, %r54;
	and.b32  	%r9, %r8, 3;
	setp.eq.s32	%p6, %r9, 0;
	@%p6 bra 	BB7_9;

	setp.eq.s32	%p7, %r9, 1;
	@%p7 bra 	BB7_8;

	setp.eq.s32	%p8, %r9, 2;
	@%p8 bra 	BB7_7;

	mul.lo.s32 	%r56, %r6, %r34;
	cvt.s64.s32	%rd8, %r56;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd4, %rd10;
	ld.global.u32 	%r57, [%rd11];
	or.b32  	%r5, %r57, %r5;
	add.s32 	%r6, %r6, 1;

BB7_7:
	mul.lo.s32 	%r58, %r6, %r34;
	cvt.s64.s32	%rd12, %r58;
	add.s64 	%rd13, %rd12, %rd2;
	shl.b64 	%rd14, %rd13, 2;
	add.s64 	%rd15, %rd4, %rd14;
	ld.global.u32 	%r59, [%rd15];
	or.b32  	%r5, %r59, %r5;
	add.s32 	%r6, %r6, 1;

BB7_8:
	mul.lo.s32 	%r60, %r6, %r34;
	cvt.s64.s32	%rd16, %r60;
	add.s64 	%rd17, %rd16, %rd2;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd19, %rd4, %rd18;
	ld.global.u32 	%r61, [%rd19];
	or.b32  	%r5, %r61, %r5;
	add.s32 	%r6, %r6, 1;
	mov.u32 	%r79, %r5;

BB7_9:
	setp.lt.u32	%p9, %r8, 4;
	@%p9 bra 	BB7_12;

	add.s32 	%r77, %r6, -1;
	shl.b32 	%r24, %r34, 2;
	mul.lo.s32 	%r76, %r6, %r34;
	mul.wide.s32 	%rd3, %r34, 4;
	mov.u32 	%r79, %r5;

BB7_11:
	cvt.s64.s32	%rd20, %r76;
	add.s64 	%rd21, %rd20, %rd2;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd23, %rd4, %rd22;
	ld.global.u32 	%r62, [%rd23];
	or.b32  	%r63, %r62, %r79;
	add.s64 	%rd24, %rd23, %rd3;
	ld.global.u32 	%r64, [%rd24];
	or.b32  	%r65, %r64, %r63;
	add.s64 	%rd25, %rd24, %rd3;
	ld.global.u32 	%r66, [%rd25];
	or.b32  	%r67, %r66, %r65;
	add.s64 	%rd26, %rd25, %rd3;
	ld.global.u32 	%r68, [%rd26];
	or.b32  	%r79, %r68, %r67;
	add.s32 	%r76, %r76, %r24;
	add.s32 	%r77, %r77, 4;
	setp.lt.s32	%p10, %r77, %r7;
	@%p10 bra 	BB7_11;

BB7_12:
	shl.b64 	%rd27, %rd1, 2;
	add.s64 	%rd28, %rd5, %rd27;
	st.global.u32 	[%rd28], %r79;

BB7_13:
	ret;
}

	// .globl	morphoErodeHor_5x5
.entry morphoErodeHor_5x5(
	.param .u64 .ptr .global .align 4 morphoErodeHor_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeHor_5x5_param_1,
	.param .u32 morphoErodeHor_5x5_param_2,
	.param .u32 morphoErodeHor_5x5_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoErodeHor_5x5_param_0];
	ld.param.u64 	%rd4, [morphoErodeHor_5x5_param_1];
	ld.param.u32 	%r8, [morphoErodeHor_5x5_param_2];
	ld.param.u32 	%r9, [morphoErodeHor_5x5_param_3];
	mov.b32	%r10, %envreg3;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %ntid.x;
	mad.lo.s32 	%r13, %r11, %r12, %r10;
	mov.u32 	%r14, %tid.x;
	add.s32 	%r1, %r13, %r14;
	rem.u32 	%r2, %r1, %r8;
	mul.lo.s32 	%r15, %r9, %r8;
	setp.ge.u32	%p1, %r1, %r15;
	@%p1 bra 	BB8_6;

	cvt.u64.u32	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32	%p2, %r2, 0;
	mov.u32 	%r36, -1;
	mov.u32 	%r35, %r36;
	@%p2 bra 	BB8_3;

	ld.global.u32 	%r35, [%rd2+-4];

BB8_3:
	add.s32 	%r18, %r8, -1;
	setp.eq.s32	%p3, %r2, %r18;
	@%p3 bra 	BB8_5;

	ld.global.u32 	%r36, [%rd2+4];

BB8_5:
	shr.u32 	%r19, %r3, 1;
	shl.b32 	%r20, %r35, 31;
	or.b32  	%r21, %r20, %r19;
	and.b32  	%r22, %r21, %r3;
	shr.u32 	%r23, %r36, 31;
	shl.b32 	%r24, %r3, 1;
	or.b32  	%r25, %r23, %r24;
	shr.u32 	%r26, %r3, 2;
	shl.b32 	%r27, %r35, 30;
	or.b32  	%r28, %r27, %r26;
	shr.u32 	%r29, %r36, 30;
	shl.b32 	%r30, %r3, 2;
	or.b32  	%r31, %r29, %r30;
	and.b32  	%r32, %r22, %r28;
	and.b32  	%r33, %r32, %r25;
	and.b32  	%r34, %r33, %r31;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r34;

BB8_6:
	ret;
}

	// .globl	morphoErodeVer_5x5
.entry morphoErodeVer_5x5(
	.param .u64 .ptr .global .align 4 morphoErodeVer_5x5_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeVer_5x5_param_1,
	.param .u32 morphoErodeVer_5x5_param_2,
	.param .u32 morphoErodeVer_5x5_param_3,
	.param .u32 morphoErodeVer_5x5_param_4,
	.param .u32 morphoErodeVer_5x5_param_5
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd2, [morphoErodeVer_5x5_param_0];
	ld.param.u64 	%rd3, [morphoErodeVer_5x5_param_1];
	ld.param.u32 	%r5, [morphoErodeVer_5x5_param_2];
	ld.param.u32 	%r6, [morphoErodeVer_5x5_param_3];
	ld.param.u32 	%r7, [morphoErodeVer_5x5_param_4];
	ld.param.u32 	%r8, [morphoErodeVer_5x5_param_5];
	mov.b32	%r9, %envreg3;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %ntid.x;
	mad.lo.s32 	%r12, %r10, %r11, %r9;
	mov.u32 	%r13, %tid.x;
	add.s32 	%r1, %r12, %r13;
	mov.u32 	%r14, %ctaid.y;
	mov.u32 	%r15, %ntid.y;
	mov.b32	%r16, %envreg4;
	mad.lo.s32 	%r17, %r14, %r15, %r16;
	mov.u32 	%r18, %tid.y;
	add.s32 	%r2, %r17, %r18;
	setp.lt.s32	%p1, %r2, %r6;
	setp.lt.s32	%p2, %r1, %r5;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB9_4;
	bra.uni 	BB9_1;

BB9_1:
	mad.lo.s32 	%r20, %r2, %r5, %r1;
	cvt.u64.u32	%rd1, %r20;
	add.s32 	%r21, %r6, -2;
	setp.lt.s32	%p4, %r2, %r21;
	setp.gt.s32	%p5, %r2, 1;
	and.pred  	%p6, %p5, %p4;
	mov.u32 	%r43, 0;
	@!%p6 bra 	BB9_3;
	bra.uni 	BB9_2;

BB9_2:
	cvt.s64.s32	%rd4, %r1;
	add.s32 	%r22, %r2, -2;
	mul.lo.s32 	%r23, %r22, %r5;
	cvt.s64.s32	%rd5, %r23;
	add.s64 	%rd6, %rd5, %rd4;
	shl.b64 	%rd7, %rd6, 2;
	add.s64 	%rd8, %rd2, %rd7;
	add.s32 	%r24, %r2, -1;
	mul.lo.s32 	%r25, %r24, %r5;
	cvt.s64.s32	%rd9, %r25;
	add.s64 	%rd10, %rd9, %rd4;
	shl.b64 	%rd11, %rd10, 2;
	add.s64 	%rd12, %rd2, %rd11;
	add.s32 	%r26, %r2, 1;
	mul.lo.s32 	%r27, %r26, %r5;
	cvt.s64.s32	%rd13, %r27;
	add.s64 	%rd14, %rd13, %rd4;
	shl.b64 	%rd15, %rd14, 2;
	add.s64 	%rd16, %rd2, %rd15;
	add.s32 	%r28, %r2, 2;
	mul.lo.s32 	%r29, %r28, %r5;
	cvt.s64.s32	%rd17, %r29;
	add.s64 	%rd18, %rd17, %rd4;
	shl.b64 	%rd19, %rd18, 2;
	add.s64 	%rd20, %rd2, %rd19;
	setp.eq.s32	%p7, %r1, 0;
	selp.b32	%r30, %r7, -1, %p7;
	add.s32 	%r31, %r5, -1;
	setp.eq.s32	%p8, %r1, %r31;
	selp.b32	%r32, %r8, -1, %p8;
	and.b32  	%r33, %r32, %r30;
	shl.b64 	%rd21, %rd1, 2;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.u32 	%r34, [%rd22];
	and.b32  	%r35, %r33, %r34;
	ld.global.u32 	%r36, [%rd8];
	and.b32  	%r37, %r35, %r36;
	ld.global.u32 	%r38, [%rd12];
	and.b32  	%r39, %r37, %r38;
	ld.global.u32 	%r40, [%rd16];
	and.b32  	%r41, %r39, %r40;
	ld.global.u32 	%r42, [%rd20];
	and.b32  	%r43, %r41, %r42;

BB9_3:
	shl.b64 	%rd23, %rd1, 2;
	add.s64 	%rd24, %rd3, %rd23;
	st.global.u32 	[%rd24], %r43;

BB9_4:
	ret;
}

	// .globl	morphoErodeHor
.entry morphoErodeHor(
	.param .u64 .ptr .global .align 4 morphoErodeHor_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeHor_param_1,
	.param .u32 morphoErodeHor_param_2,
	.param .u32 morphoErodeHor_param_3,
	.param .u32 morphoErodeHor_param_4,
	.param .u32 morphoErodeHor_param_5,
	.param .u8 morphoErodeHor_param_6,
	.param .u32 morphoErodeHor_param_7,
	.param .u32 morphoErodeHor_param_8
)
{
	.reg .pred 	%p<53>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<324>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd8, [morphoErodeHor_param_0];
	ld.param.u64 	%rd9, [morphoErodeHor_param_1];
	ld.param.u32 	%r73, [morphoErodeHor_param_2];
	ld.param.u32 	%r74, [morphoErodeHor_param_3];
	ld.param.u32 	%r75, [morphoErodeHor_param_4];
	ld.param.u32 	%r78, [morphoErodeHor_param_5];
	ld.param.u32 	%r76, [morphoErodeHor_param_7];
	ld.param.u32 	%r77, [morphoErodeHor_param_8];
	ld.param.s8 	%rs1, [morphoErodeHor_param_6];
	mov.b32	%r79, %envreg3;
	mov.u32 	%r80, %ctaid.x;
	mov.u32 	%r81, %ntid.x;
	mad.lo.s32 	%r82, %r80, %r81, %r79;
	mov.u32 	%r83, %tid.x;
	add.s32 	%r1, %r82, %r83;
	mov.u32 	%r84, %ctaid.y;
	mov.u32 	%r85, %ntid.y;
	mov.b32	%r86, %envreg4;
	mad.lo.s32 	%r87, %r84, %r85, %r86;
	mov.u32 	%r88, %tid.y;
	add.s32 	%r89, %r87, %r88;
	mul.lo.s32 	%r2, %r89, %r75;
	add.s32 	%r3, %r2, %r1;
	mul.lo.s32 	%r90, %r78, %r75;
	setp.ge.u32	%p1, %r3, %r90;
	@%p1 bra 	BB10_45;

	setp.lt.s32	%p2, %r74, 1;
	setp.lt.s32	%p3, %r73, 1;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	BB10_45;

	cvt.u64.u32	%rd1, %r3;
	mul.wide.u32 	%rd10, %r3, 4;
	add.s64 	%rd2, %rd8, %rd10;
	ld.global.u32 	%r4, [%rd2];
	and.b32  	%r5, %r74, 31;
	and.b32  	%r92, %r73, 31;
	setp.eq.s32	%p5, %r92, 0;
	selp.b32	%r6, 31, %r92, %p5;
	setp.ne.s32	%p6, %r92, 0;
	selp.u32	%r93, 1, 0, %p6;
	shr.s32 	%r94, %r73, 5;
	add.s32 	%r7, %r93, %r94;
	sub.s32 	%r8, %r1, %r7;
	add.s32 	%r9, %r1, %r7;
	setp.eq.s32	%p7, %r1, 0;
	mov.u32 	%r297, -1;
	mov.u32 	%r296, %r297;
	@%p7 bra 	BB10_4;

	ld.global.u32 	%r296, [%rd2+-4];

BB10_4:
	add.s32 	%r12, %r75, -1;
	setp.eq.s32	%p8, %r1, %r12;
	@%p8 bra 	BB10_6;

	ld.global.u32 	%r297, [%rd2+4];

BB10_6:
	add.s32 	%r101, %r6, 1;
	setp.gt.u32	%p9, %r101, 2;
	selp.b32	%r15, %r6, 1, %p9;
	and.b32  	%r100, %r15, 3;
	mov.u32 	%r298, 1;
	mov.u32 	%r320, 0;
	setp.eq.s32	%p10, %r100, 0;
	@%p10 bra 	BB10_7;

	setp.eq.s32	%p11, %r100, 1;
	@%p11 bra 	BB10_9;
	bra.uni 	BB10_10;

BB10_9:
	mov.u32 	%r301, %r4;
	bra.uni 	BB10_13;

BB10_7:
	mov.u32 	%r304, %r4;
	bra.uni 	BB10_14;

BB10_10:
	setp.eq.s32	%p12, %r100, 2;
	mov.u32 	%r299, %r4;
	@%p12 bra 	BB10_12;

	setp.eq.s32	%p13, %r6, %r5;
	shr.u32 	%r103, %r4, 1;
	shl.b32 	%r104, %r296, 31;
	or.b32  	%r105, %r104, %r103;
	and.b32  	%r106, %r105, %r4;
	setp.ne.s32	%p14, %r6, 1;
	or.pred  	%p15, %p14, %p13;
	shr.u32 	%r107, %r297, 31;
	shl.b32 	%r108, %r4, 1;
	or.b32  	%r109, %r108, %r107;
	selp.b32	%r110, %r109, -1, %p15;
	and.b32  	%r299, %r106, %r110;
	mov.u32 	%r298, 2;

BB10_12:
	neg.s32 	%r111, %r298;
	and.b32  	%r112, %r111, 31;
	shl.b32 	%r113, %r296, %r112;
	shr.u32 	%r114, %r4, %r298;
	or.b32  	%r115, %r113, %r114;
	and.b32  	%r116, %r115, %r299;
	setp.ne.s32	%p16, %r298, %r6;
	setp.eq.s32	%p17, %r6, %r5;
	or.pred  	%p18, %p16, %p17;
	shr.u32 	%r117, %r297, %r112;
	shl.b32 	%r118, %r4, %r298;
	or.b32  	%r119, %r118, %r117;
	selp.b32	%r120, %r119, -1, %p18;
	and.b32  	%r301, %r116, %r120;
	add.s32 	%r298, %r298, 1;

BB10_13:
	neg.s32 	%r121, %r298;
	and.b32  	%r122, %r121, 31;
	shl.b32 	%r123, %r296, %r122;
	and.b32  	%r124, %r298, 31;
	shr.u32 	%r125, %r4, %r124;
	or.b32  	%r126, %r123, %r125;
	and.b32  	%r127, %r126, %r301;
	setp.ne.s32	%p19, %r298, %r6;
	setp.eq.s32	%p20, %r6, %r5;
	or.pred  	%p21, %p19, %p20;
	shl.b32 	%r128, %r4, %r124;
	shr.u32 	%r129, %r297, %r122;
	or.b32  	%r130, %r128, %r129;
	selp.b32	%r131, %r130, -1, %p21;
	and.b32  	%r320, %r127, %r131;
	add.s32 	%r298, %r298, 1;
	mov.u32 	%r304, %r320;

BB10_14:
	setp.lt.u32	%p22, %r15, 4;
	@%p22 bra 	BB10_17;

	mov.u32 	%r320, %r304;

BB10_16:
	neg.s32 	%r132, %r298;
	and.b32  	%r133, %r132, 31;
	shl.b32 	%r134, %r296, %r133;
	and.b32  	%r135, %r298, 31;
	shr.u32 	%r136, %r4, %r135;
	or.b32  	%r137, %r134, %r136;
	and.b32  	%r138, %r137, %r320;
	setp.ne.s32	%p23, %r298, %r6;
	setp.eq.s32	%p24, %r6, %r5;
	or.pred  	%p25, %p23, %p24;
	shl.b32 	%r139, %r4, %r135;
	shr.u32 	%r140, %r297, %r133;
	or.b32  	%r141, %r139, %r140;
	selp.b32	%r142, %r141, -1, %p25;
	and.b32  	%r143, %r138, %r142;
	not.b32 	%r144, %r298;
	and.b32  	%r145, %r144, 31;
	shl.b32 	%r146, %r296, %r145;
	add.s32 	%r147, %r298, 1;
	and.b32  	%r148, %r147, 31;
	shr.u32 	%r149, %r4, %r148;
	or.b32  	%r150, %r146, %r149;
	and.b32  	%r151, %r150, %r143;
	setp.ne.s32	%p26, %r147, %r6;
	or.pred  	%p27, %p26, %p24;
	shl.b32 	%r152, %r4, %r148;
	shr.u32 	%r153, %r297, %r145;
	or.b32  	%r154, %r152, %r153;
	selp.b32	%r155, %r154, -1, %p27;
	and.b32  	%r156, %r151, %r155;
	mov.u32 	%r157, -2;
	sub.s32 	%r158, %r157, %r298;
	and.b32  	%r159, %r158, 31;
	shl.b32 	%r160, %r296, %r159;
	add.s32 	%r161, %r298, 2;
	and.b32  	%r162, %r161, 31;
	shr.u32 	%r163, %r4, %r162;
	or.b32  	%r164, %r160, %r163;
	and.b32  	%r165, %r164, %r156;
	setp.ne.s32	%p28, %r161, %r6;
	or.pred  	%p29, %p28, %p24;
	shl.b32 	%r166, %r4, %r162;
	shr.u32 	%r167, %r297, %r159;
	or.b32  	%r168, %r166, %r167;
	selp.b32	%r169, %r168, -1, %p29;
	and.b32  	%r170, %r165, %r169;
	mov.u32 	%r171, -3;
	sub.s32 	%r172, %r171, %r298;
	and.b32  	%r173, %r172, 31;
	shl.b32 	%r174, %r296, %r173;
	add.s32 	%r175, %r298, 3;
	and.b32  	%r176, %r175, 31;
	shr.u32 	%r177, %r4, %r176;
	or.b32  	%r178, %r174, %r177;
	and.b32  	%r179, %r178, %r170;
	setp.ne.s32	%p30, %r175, %r6;
	or.pred  	%p31, %p30, %p24;
	shl.b32 	%r180, %r4, %r176;
	shr.u32 	%r181, %r297, %r173;
	or.b32  	%r182, %r180, %r181;
	selp.b32	%r183, %r182, -1, %p31;
	and.b32  	%r320, %r179, %r183;
	add.s32 	%r298, %r298, 4;
	setp.le.u32	%p32, %r298, %r6;
	@%p32 bra 	BB10_16;

BB10_17:
	shl.b64 	%rd11, %rd1, 2;
	add.s64 	%rd3, %rd9, %rd11;
	setp.eq.s32	%p33, %r7, 1;
	@%p33 bra 	BB10_42;
	bra.uni 	BB10_18;

BB10_42:
	setp.eq.s32	%p48, %r73, 32;
	selp.b32	%r290, %r296, -1, %p48;
	setp.eq.s32	%p49, %r74, 32;
	selp.b32	%r291, %r297, -1, %p49;
	and.b32  	%r292, %r291, %r290;
	and.b32  	%r323, %r292, %r320;
	and.b16  	%rs3, %rs1, 255;
	setp.eq.s16	%p50, %rs3, 0;
	@%p50 bra 	BB10_44;

	selp.b32	%r293, %r76, -1, %p7;
	selp.b32	%r294, %r77, -1, %p8;
	and.b32  	%r295, %r294, %r293;
	and.b32  	%r323, %r295, %r323;

BB10_44:
	st.global.u32 	[%rd3], %r323;
	bra.uni 	BB10_45;

BB10_18:
	setp.lt.s32	%p34, %r8, 0;
	mov.u32 	%r309, -1;
	mov.u32 	%r311, %r309;
	@%p34 bra 	BB10_20;

	cvt.s64.s32	%rd12, %r2;
	cvt.s64.s32	%rd13, %r8;
	add.s64 	%rd14, %rd12, %rd13;
	shl.b64 	%rd15, %rd14, 2;
	add.s64 	%rd16, %rd8, %rd15;
	ld.global.u32 	%r311, [%rd16];

BB10_20:
	setp.ge.s32	%p35, %r9, %r75;
	@%p35 bra 	BB10_22;

	cvt.s64.s32	%rd17, %r2;
	cvt.s64.s32	%rd18, %r9;
	add.s64 	%rd19, %rd17, %rd18;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd8, %rd20;
	ld.global.u32 	%r309, [%rd21];

BB10_22:
	setp.lt.u32	%p36, %r7, 2;
	@%p36 bra 	BB10_35;

	neg.s32 	%r187, %r6;
	and.b32  	%r37, %r187, 31;
	cvt.s64.s32	%rd22, %r9;
	cvt.s64.s32	%rd23, %r2;
	add.s64 	%rd24, %rd23, %rd22;
	neg.s32 	%r188, %r74;
	and.b32  	%r38, %r188, 31;
	add.s64 	%rd4, %rd24, -1;
	cvt.s64.s32	%rd25, %r8;
	add.s64 	%rd5, %rd23, %rd25;
	mov.u32 	%r310, 1;

BB10_24:
	add.s32 	%r43, %r310, %r8;
	setp.lt.s32	%p37, %r43, 0;
	cvt.s64.s32	%rd26, %r310;
	add.s64 	%rd27, %rd5, %rd26;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd6, %rd8, %rd28;
	mov.u32 	%r317, -1;
	mov.u32 	%r314, %r317;
	@%p37 bra 	BB10_26;

	ld.global.u32 	%r314, [%rd6];

BB10_26:
	shr.u32 	%r191, %r314, %r6;
	shl.b32 	%r192, %r311, %r37;
	or.b32  	%r46, %r191, %r192;
	add.s32 	%r193, %r43, 1;
	setp.lt.s32	%p38, %r193, 0;
	mov.u32 	%r311, %r317;
	@%p38 bra 	BB10_28;

	ld.global.u32 	%r311, [%rd6+4];

BB10_28:
	shr.u32 	%r195, %r311, %r6;
	shl.b32 	%r196, %r314, %r37;
	or.b32  	%r49, %r195, %r196;
	sub.s32 	%r50, %r9, %r310;
	sub.s64 	%rd30, %rd4, %rd26;
	shl.b64 	%rd31, %rd30, 2;
	add.s64 	%rd7, %rd8, %rd31;
	setp.ge.s32	%p39, %r50, %r75;
	mov.u32 	%r316, %r317;
	@%p39 bra 	BB10_30;

	ld.global.u32 	%r316, [%rd7+4];

BB10_30:
	shr.u32 	%r198, %r309, %r38;
	shl.b32 	%r199, %r316, %r5;
	or.b32  	%r53, %r199, %r198;
	add.s32 	%r200, %r50, -1;
	setp.ge.s32	%p40, %r200, %r75;
	@%p40 bra 	BB10_32;

	ld.global.u32 	%r317, [%rd7];

BB10_32:
	shr.u32 	%r203, %r316, %r38;
	shl.b32 	%r204, %r317, %r5;
	or.b32  	%r56, %r204, %r203;
	mov.u32 	%r318, 0;
	mov.u32 	%r319, %r318;

BB10_33:
	and.b32  	%r205, %r318, 24;
	shr.u32 	%r206, %r49, %r205;
	shl.b32 	%r207, %r46, %r319;
	or.b32  	%r208, %r207, %r206;
	and.b32  	%r209, %r208, %r320;
	shr.u32 	%r210, %r53, %r205;
	shl.b32 	%r211, %r56, %r319;
	or.b32  	%r212, %r211, %r210;
	and.b32  	%r213, %r209, %r212;
	add.s32 	%r214, %r319, 1;
	shl.b32 	%r215, %r46, %r214;
	add.s32 	%r216, %r318, 31;
	shr.u32 	%r217, %r49, %r216;
	or.b32  	%r218, %r215, %r217;
	and.b32  	%r219, %r218, %r213;
	shl.b32 	%r220, %r56, %r214;
	shr.u32 	%r221, %r53, %r216;
	or.b32  	%r222, %r220, %r221;
	and.b32  	%r223, %r219, %r222;
	add.s32 	%r224, %r319, 2;
	shl.b32 	%r225, %r46, %r224;
	add.s32 	%r226, %r318, 30;
	shr.u32 	%r227, %r49, %r226;
	or.b32  	%r228, %r225, %r227;
	and.b32  	%r229, %r228, %r223;
	shl.b32 	%r230, %r56, %r224;
	shr.u32 	%r231, %r53, %r226;
	or.b32  	%r232, %r230, %r231;
	and.b32  	%r233, %r229, %r232;
	add.s32 	%r234, %r319, 3;
	shl.b32 	%r235, %r46, %r234;
	add.s32 	%r236, %r318, 29;
	shr.u32 	%r237, %r49, %r236;
	or.b32  	%r238, %r235, %r237;
	and.b32  	%r239, %r238, %r233;
	shl.b32 	%r240, %r56, %r234;
	shr.u32 	%r241, %r53, %r236;
	or.b32  	%r242, %r240, %r241;
	and.b32  	%r243, %r239, %r242;
	add.s32 	%r244, %r319, 4;
	shl.b32 	%r245, %r46, %r244;
	add.s32 	%r246, %r318, 28;
	shr.u32 	%r247, %r49, %r246;
	or.b32  	%r248, %r245, %r247;
	and.b32  	%r249, %r248, %r243;
	shl.b32 	%r250, %r56, %r244;
	shr.u32 	%r251, %r53, %r246;
	or.b32  	%r252, %r250, %r251;
	and.b32  	%r253, %r249, %r252;
	add.s32 	%r254, %r319, 5;
	shl.b32 	%r255, %r46, %r254;
	add.s32 	%r256, %r318, 27;
	shr.u32 	%r257, %r49, %r256;
	or.b32  	%r258, %r255, %r257;
	and.b32  	%r259, %r258, %r253;
	shl.b32 	%r260, %r56, %r254;
	shr.u32 	%r261, %r53, %r256;
	or.b32  	%r262, %r260, %r261;
	and.b32  	%r263, %r259, %r262;
	add.s32 	%r264, %r319, 6;
	shl.b32 	%r265, %r46, %r264;
	add.s32 	%r266, %r318, 26;
	shr.u32 	%r267, %r49, %r266;
	or.b32  	%r268, %r265, %r267;
	and.b32  	%r269, %r268, %r263;
	shl.b32 	%r270, %r56, %r264;
	shr.u32 	%r271, %r53, %r266;
	or.b32  	%r272, %r270, %r271;
	and.b32  	%r273, %r269, %r272;
	add.s32 	%r274, %r319, 7;
	shl.b32 	%r275, %r46, %r274;
	add.s32 	%r276, %r318, 25;
	shr.u32 	%r277, %r49, %r276;
	or.b32  	%r278, %r275, %r277;
	and.b32  	%r279, %r278, %r273;
	shl.b32 	%r280, %r56, %r274;
	shr.u32 	%r281, %r53, %r276;
	or.b32  	%r282, %r280, %r281;
	and.b32  	%r320, %r279, %r282;
	add.s32 	%r318, %r318, -8;
	add.s32 	%r319, %r319, 8;
	setp.ne.s32	%p41, %r319, 32;
	@%p41 bra 	BB10_33;

	and.b32  	%r283, %r49, %r46;
	and.b32  	%r284, %r283, %r53;
	and.b32  	%r285, %r284, %r56;
	and.b32  	%r320, %r285, %r320;
	add.s32 	%r310, %r310, 1;
	setp.lt.u32	%p42, %r310, %r7;
	mov.u32 	%r309, %r317;
	@%p42 bra 	BB10_24;

BB10_35:
	and.b16  	%rs2, %rs1, 255;
	setp.eq.s16	%p43, %rs2, 0;
	@%p43 bra 	BB10_36;

	add.s32 	%r66, %r7, -1;
	setp.lt.u32	%p44, %r1, %r66;
	mov.u32 	%r322, 0;
	@%p44 bra 	BB10_41;

	setp.eq.s32	%p45, %r1, %r66;
	@%p45 bra 	BB10_40;
	bra.uni 	BB10_39;

BB10_40:
	and.b32  	%r322, %r320, %r76;
	bra.uni 	BB10_41;

BB10_36:
	mov.u32 	%r322, %r320;
	bra.uni 	BB10_41;

BB10_39:
	sub.s32 	%r287, %r75, %r7;
	setp.gt.u32	%p46, %r1, %r287;
	setp.eq.s32	%p47, %r1, %r287;
	selp.b32	%r288, %r77, -1, %p47;
	and.b32  	%r289, %r320, %r288;
	selp.b32	%r322, 0, %r289, %p46;

BB10_41:
	st.global.u32 	[%rd3], %r322;

BB10_45:
	ret;
}

	// .globl	morphoErodeHor_32word
.entry morphoErodeHor_32word(
	.param .u64 .ptr .global .align 4 morphoErodeHor_32word_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeHor_32word_param_1,
	.param .u32 morphoErodeHor_32word_param_2,
	.param .u32 morphoErodeHor_32word_param_3,
	.param .u32 morphoErodeHor_32word_param_4,
	.param .u8 morphoErodeHor_32word_param_5,
	.param .u32 morphoErodeHor_32word_param_6,
	.param .u32 morphoErodeHor_32word_param_7,
	.param .u8 morphoErodeHor_32word_param_8
)
{
	.reg .pred 	%p<31>;
	.reg .b16 	%rs<8>;
	.reg .b32 	%r<148>;
	.reg .b64 	%rd<8>;


	ld.param.u64 	%rd3, [morphoErodeHor_32word_param_0];
	ld.param.u64 	%rd4, [morphoErodeHor_32word_param_1];
	ld.param.u32 	%r26, [morphoErodeHor_32word_param_2];
	ld.param.u32 	%r27, [morphoErodeHor_32word_param_3];
	ld.param.u32 	%r30, [morphoErodeHor_32word_param_4];
	ld.param.u32 	%r28, [morphoErodeHor_32word_param_6];
	ld.param.u32 	%r29, [morphoErodeHor_32word_param_7];
	ld.param.s8 	%rs2, [morphoErodeHor_32word_param_8];
	ld.param.s8 	%rs1, [morphoErodeHor_32word_param_5];
	mov.b32	%r31, %envreg3;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %ntid.x;
	mad.lo.s32 	%r34, %r32, %r33, %r31;
	mov.u32 	%r35, %tid.x;
	add.s32 	%r1, %r34, %r35;
	mov.u32 	%r36, %ctaid.y;
	mov.u32 	%r37, %ntid.y;
	mov.b32	%r38, %envreg4;
	mad.lo.s32 	%r39, %r36, %r37, %r38;
	mov.u32 	%r40, %tid.y;
	add.s32 	%r41, %r39, %r40;
	mad.lo.s32 	%r2, %r41, %r27, %r1;
	mul.lo.s32 	%r42, %r30, %r27;
	setp.ge.u32	%p1, %r2, %r42;
	@%p1 bra 	BB11_19;

	cvt.u64.u32	%rd1, %r2;
	mul.wide.u32 	%rd5, %r2, 4;
	add.s64 	%rd2, %rd3, %rd5;
	ld.global.u32 	%r3, [%rd2];
	setp.eq.s32	%p2, %r1, 0;
	mov.u32 	%r137, -1;
	mov.u32 	%r136, %r137;
	@%p2 bra 	BB11_3;

	ld.global.u32 	%r136, [%rd2+-4];

BB11_3:
	add.s32 	%r6, %r27, -1;
	setp.eq.s32	%p3, %r1, %r6;
	@%p3 bra 	BB11_5;

	ld.global.u32 	%r137, [%rd2+4];

BB11_5:
	setp.lt.s32	%p4, %r26, 1;
	@%p4 bra 	BB11_6;

	and.b32  	%r49, %r26, 3;
	mov.u32 	%r138, 1;
	mov.u32 	%r147, 0;
	setp.eq.s32	%p5, %r49, 0;
	@%p5 bra 	BB11_8;

	setp.eq.s32	%p6, %r49, 1;
	@%p6 bra 	BB11_10;
	bra.uni 	BB11_11;

BB11_10:
	mov.u32 	%r141, %r3;
	bra.uni 	BB11_14;

BB11_6:
	mov.u32 	%r147, %r3;
	bra.uni 	BB11_18;

BB11_8:
	mov.u32 	%r143, %r3;
	bra.uni 	BB11_15;

BB11_11:
	setp.eq.s32	%p7, %r49, 2;
	mov.u32 	%r139, %r3;
	@%p7 bra 	BB11_13;

	and.b16  	%rs3, %rs2, 255;
	setp.eq.s16	%p8, %rs3, 0;
	shr.u32 	%r51, %r3, 1;
	shl.b32 	%r52, %r136, 31;
	or.b32  	%r53, %r52, %r51;
	and.b32  	%r54, %r53, %r3;
	setp.ne.s32	%p9, %r26, 1;
	or.pred  	%p10, %p9, %p8;
	shr.u32 	%r55, %r137, 31;
	shl.b32 	%r56, %r3, 1;
	or.b32  	%r57, %r56, %r55;
	selp.b32	%r58, %r57, -1, %p10;
	and.b32  	%r139, %r54, %r58;
	mov.u32 	%r138, 2;

BB11_13:
	neg.s32 	%r59, %r138;
	and.b32  	%r60, %r59, 31;
	shl.b32 	%r61, %r136, %r60;
	shr.u32 	%r62, %r3, %r138;
	or.b32  	%r63, %r61, %r62;
	and.b32  	%r64, %r63, %r139;
	setp.ne.s32	%p11, %r138, %r26;
	and.b16  	%rs4, %rs2, 255;
	setp.eq.s16	%p12, %rs4, 0;
	or.pred  	%p13, %p11, %p12;
	shr.u32 	%r65, %r137, %r60;
	shl.b32 	%r66, %r3, %r138;
	or.b32  	%r67, %r66, %r65;
	selp.b32	%r68, %r67, -1, %p13;
	and.b32  	%r141, %r64, %r68;
	add.s32 	%r138, %r138, 1;

BB11_14:
	neg.s32 	%r69, %r138;
	and.b32  	%r70, %r69, 31;
	shl.b32 	%r71, %r136, %r70;
	and.b32  	%r72, %r138, 31;
	shr.u32 	%r73, %r3, %r72;
	or.b32  	%r74, %r71, %r73;
	and.b32  	%r75, %r74, %r141;
	setp.ne.s32	%p14, %r138, %r26;
	and.b16  	%rs5, %rs2, 255;
	setp.eq.s16	%p15, %rs5, 0;
	or.pred  	%p16, %p14, %p15;
	shl.b32 	%r76, %r3, %r72;
	shr.u32 	%r77, %r137, %r70;
	or.b32  	%r78, %r76, %r77;
	selp.b32	%r79, %r78, -1, %p16;
	and.b32  	%r143, %r75, %r79;
	add.s32 	%r138, %r138, 1;
	mov.u32 	%r147, %r143;

BB11_15:
	setp.lt.u32	%p17, %r26, 4;
	@%p17 bra 	BB11_18;

	mov.u32 	%r147, %r143;

BB11_17:
	neg.s32 	%r80, %r138;
	and.b32  	%r81, %r80, 31;
	shl.b32 	%r82, %r136, %r81;
	and.b32  	%r83, %r138, 31;
	shr.u32 	%r84, %r3, %r83;
	or.b32  	%r85, %r82, %r84;
	and.b32  	%r86, %r85, %r147;
	setp.ne.s32	%p18, %r138, %r26;
	and.b16  	%rs6, %rs2, 255;
	setp.eq.s16	%p19, %rs6, 0;
	or.pred  	%p20, %p18, %p19;
	shl.b32 	%r87, %r3, %r83;
	shr.u32 	%r88, %r137, %r81;
	or.b32  	%r89, %r87, %r88;
	selp.b32	%r90, %r89, -1, %p20;
	and.b32  	%r91, %r86, %r90;
	not.b32 	%r92, %r138;
	and.b32  	%r93, %r92, 31;
	shl.b32 	%r94, %r136, %r93;
	add.s32 	%r95, %r138, 1;
	and.b32  	%r96, %r95, 31;
	shr.u32 	%r97, %r3, %r96;
	or.b32  	%r98, %r94, %r97;
	and.b32  	%r99, %r98, %r91;
	setp.ne.s32	%p21, %r95, %r26;
	or.pred  	%p22, %p21, %p19;
	shl.b32 	%r100, %r3, %r96;
	shr.u32 	%r101, %r137, %r93;
	or.b32  	%r102, %r100, %r101;
	selp.b32	%r103, %r102, -1, %p22;
	and.b32  	%r104, %r99, %r103;
	mov.u32 	%r105, -2;
	sub.s32 	%r106, %r105, %r138;
	and.b32  	%r107, %r106, 31;
	shl.b32 	%r108, %r136, %r107;
	add.s32 	%r109, %r138, 2;
	and.b32  	%r110, %r109, 31;
	shr.u32 	%r111, %r3, %r110;
	or.b32  	%r112, %r108, %r111;
	and.b32  	%r113, %r112, %r104;
	setp.ne.s32	%p23, %r109, %r26;
	or.pred  	%p24, %p23, %p19;
	shl.b32 	%r114, %r3, %r110;
	shr.u32 	%r115, %r137, %r107;
	or.b32  	%r116, %r114, %r115;
	selp.b32	%r117, %r116, -1, %p24;
	and.b32  	%r118, %r113, %r117;
	mov.u32 	%r119, -3;
	sub.s32 	%r120, %r119, %r138;
	and.b32  	%r121, %r120, 31;
	shl.b32 	%r122, %r136, %r121;
	add.s32 	%r123, %r138, 3;
	and.b32  	%r124, %r123, 31;
	shr.u32 	%r125, %r3, %r124;
	or.b32  	%r126, %r122, %r125;
	and.b32  	%r127, %r126, %r118;
	setp.ne.s32	%p25, %r123, %r26;
	or.pred  	%p26, %p25, %p19;
	shl.b32 	%r128, %r3, %r124;
	shr.u32 	%r129, %r137, %r121;
	or.b32  	%r130, %r128, %r129;
	selp.b32	%r131, %r130, -1, %p26;
	and.b32  	%r147, %r127, %r131;
	add.s32 	%r138, %r138, 4;
	setp.lt.s32	%p27, %r123, %r26;
	@%p27 bra 	BB11_17;

BB11_18:
	selp.b32	%r132, %r29, -1, %p3;
	selp.b32	%r133, %r28, %r132, %p2;
	and.b16  	%rs7, %rs1, 255;
	setp.eq.s16	%p30, %rs7, 0;
	selp.b32	%r134, -1, %r133, %p30;
	and.b32  	%r135, %r147, %r134;
	shl.b64 	%rd6, %rd1, 2;
	add.s64 	%rd7, %rd4, %rd6;
	st.global.u32 	[%rd7], %r135;

BB11_19:
	ret;
}

	// .globl	morphoErodeVer
.entry morphoErodeVer(
	.param .u64 .ptr .global .align 4 morphoErodeVer_param_0,
	.param .u64 .ptr .global .align 4 morphoErodeVer_param_1,
	.param .u32 morphoErodeVer_param_2,
	.param .u32 morphoErodeVer_param_3,
	.param .u32 morphoErodeVer_param_4,
	.param .u8 morphoErodeVer_param_5,
	.param .u32 morphoErodeVer_param_6
)
{
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<92>;
	.reg .b64 	%rd<29>;


	ld.param.u64 	%rd4, [morphoErodeVer_param_0];
	ld.param.u64 	%rd5, [morphoErodeVer_param_1];
	ld.param.u32 	%r35, [morphoErodeVer_param_2];
	ld.param.u32 	%r36, [morphoErodeVer_param_3];
	ld.param.u32 	%r37, [morphoErodeVer_param_4];
	ld.param.u32 	%r38, [morphoErodeVer_param_6];
	ld.param.s8 	%rs1, [morphoErodeVer_param_5];
	mov.b32	%r39, %envreg3;
	mov.u32 	%r40, %ctaid.x;
	mov.u32 	%r41, %ntid.x;
	mad.lo.s32 	%r42, %r40, %r41, %r39;
	mov.u32 	%r43, %tid.x;
	add.s32 	%r1, %r42, %r43;
	mov.u32 	%r44, %ctaid.y;
	mov.u32 	%r45, %ntid.y;
	mov.b32	%r46, %envreg4;
	mad.lo.s32 	%r2, %r44, %r45, %r46;
	mov.u32 	%r3, %tid.y;
	add.s32 	%r4, %r2, %r3;
	setp.lt.s32	%p1, %r4, %r37;
	setp.lt.s32	%p2, %r1, %r36;
	and.pred  	%p3, %p1, %p2;
	@!%p3 bra 	BB12_15;
	bra.uni 	BB12_1;

BB12_1:
	mad.lo.s32 	%r47, %r4, %r36, %r1;
	cvt.u64.u32	%rd1, %r47;
	mul.wide.u32 	%rd6, %r47, 4;
	add.s64 	%rd7, %rd4, %rd6;
	ld.global.u32 	%r5, [%rd7];
	sub.s32 	%r48, %r4, %r35;
	mov.u32 	%r90, 0;
	max.s32 	%r6, %r90, %r48;
	sub.s32 	%r50, %r37, %r38;
	setp.lt.s32	%p4, %r4, %r50;
	add.s32 	%r51, %r4, %r38;
	add.s32 	%r52, %r37, -1;
	selp.b32	%r53, %r51, %r52, %p4;
	setp.gt.s32	%p5, %r6, %r53;
	@%p5 bra 	BB12_2;
	bra.uni 	BB12_3;

BB12_2:
	mov.u32 	%r90, %r5;
	bra.uni 	BB12_12;

BB12_3:
	cvt.s64.s32	%rd2, %r1;
	add.s32 	%r59, %r53, 1;
	max.s32 	%r62, %r48, %r90;
	sub.s32 	%r7, %r59, %r62;
	and.b32  	%r8, %r7, 3;
	setp.eq.s32	%p7, %r8, 0;
	@%p7 bra 	BB12_9;

	setp.eq.s32	%p8, %r8, 1;
	@%p8 bra 	BB12_8;

	setp.eq.s32	%p9, %r8, 2;
	@%p9 bra 	BB12_7;

	mul.lo.s32 	%r63, %r6, %r36;
	cvt.s64.s32	%rd8, %r63;
	add.s64 	%rd9, %rd8, %rd2;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd4, %rd10;
	ld.global.u32 	%r64, [%rd11];
	and.b32  	%r5, %r64, %r5;
	add.s32 	%r6, %r6, 1;

BB12_7:
	mul.lo.s32 	%r65, %r6, %r36;
	cvt.s64.s32	%rd12, %r65;
	add.s64 	%rd13, %rd12, %rd2;
	shl.b64 	%rd14, %rd13, 2;
	add.s64 	%rd15, %rd4, %rd14;
	ld.global.u32 	%r66, [%rd15];
	and.b32  	%r5, %r66, %r5;
	add.s32 	%r6, %r6, 1;

BB12_8:
	mul.lo.s32 	%r67, %r6, %r36;
	cvt.s64.s32	%rd16, %r67;
	add.s64 	%rd17, %rd16, %rd2;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd19, %rd4, %rd18;
	ld.global.u32 	%r68, [%rd19];
	and.b32  	%r5, %r68, %r5;
	add.s32 	%r6, %r6, 1;
	mov.u32 	%r90, %r5;

BB12_9:
	setp.lt.u32	%p10, %r7, 4;
	@%p10 bra 	BB12_12;

	add.s32 	%r88, %r6, -1;
	shl.b32 	%r23, %r36, 2;
	mul.lo.s32 	%r87, %r6, %r36;
	mul.wide.s32 	%rd3, %r36, 4;
	mov.u32 	%r90, %r5;

BB12_11:
	cvt.s64.s32	%rd20, %r87;
	add.s64 	%rd21, %rd20, %rd2;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd23, %rd4, %rd22;
	ld.global.u32 	%r72, [%rd23];
	and.b32  	%r73, %r72, %r90;
	add.s64 	%rd24, %rd23, %rd3;
	ld.global.u32 	%r74, [%rd24];
	and.b32  	%r75, %r74, %r73;
	add.s64 	%rd25, %rd24, %rd3;
	ld.global.u32 	%r76, [%rd25];
	and.b32  	%r77, %r76, %r75;
	add.s64 	%rd26, %rd25, %rd3;
	ld.global.u32 	%r78, [%rd26];
	and.b32  	%r90, %r78, %r77;
	add.s32 	%r87, %r87, %r23;
	add.s32 	%r88, %r88, 4;
	setp.lt.s32	%p12, %r88, %r53;
	@%p12 bra 	BB12_11;

BB12_12:
	and.b16  	%rs2, %rs1, 255;
	setp.eq.s16	%p13, %rs2, 0;
	@%p13 bra 	BB12_14;

	setp.ge.s32	%p14, %r4, %r35;
	sub.s32 	%r79, %r37, %r4;
	setp.gt.s32	%p15, %r79, %r38;
	and.pred  	%p16, %p14, %p15;
	selp.b32	%r90, %r90, 0, %p16;

BB12_14:
	shl.b64 	%rd27, %rd1, 2;
	add.s64 	%rd28, %rd5, %rd27;
	st.global.u32 	[%rd28], %r90;

BB12_15:
	ret;
}

	// .globl	kernel_HistogramRectAllChannels
.entry kernel_HistogramRectAllChannels(
	.param .u64 .ptr .global .align 8 kernel_HistogramRectAllChannels_param_0,
	.param .u32 kernel_HistogramRectAllChannels_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectAllChannels_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd5, [kernel_HistogramRectAllChannels_param_0];
	ld.param.u32 	%r8, [kernel_HistogramRectAllChannels_param_1];
	ld.param.u64 	%rd6, [kernel_HistogramRectAllChannels_param_2];
	mov.b32	%r9, %envreg3;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mad.lo.s32 	%r11, %r10, %r1, %r9;
	mov.u32 	%r12, %tid.x;
	add.s32 	%r2, %r11, %r12;
	and.b32  	%r3, %r2, 255;
	bfe.u32 	%r4, %r8, 1, 29;
	setp.ge.u32	%p1, %r2, %r4;
	@%p1 bra 	BB13_3;

	cvt.s64.s32	%rd26, %r2;
	add.s32 	%r5, %r3, 65536;
	add.s32 	%r6, %r3, 131072;
	add.s32 	%r7, %r3, 196608;
	mov.b32	%r13, %envreg6;
	mul.lo.s32 	%r14, %r1, %r13;
	cvt.s64.s32	%rd2, %r14;

BB13_2:
	and.b64  	%rd7, %rd26, 4294967295;
	shl.b64 	%rd8, %rd7, 3;
	add.s64 	%rd9, %rd5, %rd8;
	ld.global.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [%rd9];
	mul.wide.u16 	%r15, %rs1, 256;
	add.s32 	%r16, %r15, %r3;
	mul.wide.s32 	%rd10, %r16, 4;
	add.s64 	%rd11, %rd6, %rd10;
	ld.global.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [%rd9+4];
	atom.global.add.u32 	%r17, [%rd11], 1;
	mul.wide.u16 	%r18, %rs5, 256;
	add.s32 	%r19, %r18, %r3;
	mul.wide.s32 	%rd12, %r19, 4;
	add.s64 	%rd13, %rd6, %rd12;
	atom.global.add.u32 	%r20, [%rd13], 1;
	mul.wide.u16 	%r21, %rs2, 256;
	add.s32 	%r22, %r5, %r21;
	mul.wide.s32 	%rd14, %r22, 4;
	add.s64 	%rd15, %rd6, %rd14;
	atom.global.add.u32 	%r23, [%rd15], 1;
	mul.wide.u16 	%r24, %rs6, 256;
	add.s32 	%r25, %r5, %r24;
	mul.wide.s32 	%rd16, %r25, 4;
	add.s64 	%rd17, %rd6, %rd16;
	atom.global.add.u32 	%r26, [%rd17], 1;
	mul.wide.u16 	%r27, %rs3, 256;
	add.s32 	%r28, %r6, %r27;
	mul.wide.s32 	%rd18, %r28, 4;
	add.s64 	%rd19, %rd6, %rd18;
	atom.global.add.u32 	%r29, [%rd19], 1;
	mul.wide.u16 	%r30, %rs7, 256;
	add.s32 	%r31, %r6, %r30;
	mul.wide.s32 	%rd20, %r31, 4;
	add.s64 	%rd21, %rd6, %rd20;
	atom.global.add.u32 	%r32, [%rd21], 1;
	mul.wide.u16 	%r33, %rs4, 256;
	add.s32 	%r34, %r7, %r33;
	mul.wide.s32 	%rd22, %r34, 4;
	add.s64 	%rd23, %rd6, %rd22;
	atom.global.add.u32 	%r35, [%rd23], 1;
	mul.wide.u16 	%r36, %rs8, 256;
	add.s32 	%r37, %r7, %r36;
	mul.wide.s32 	%rd24, %r37, 4;
	add.s64 	%rd25, %rd6, %rd24;
	atom.global.add.u32 	%r38, [%rd25], 1;
	add.s64 	%rd26, %rd2, %rd7;
	cvt.u32.u64	%r39, %rd26;
	setp.lt.u32	%p2, %r39, %r4;
	@%p2 bra 	BB13_2;

BB13_3:
	ret;
}

	// .globl	kernel_HistogramRectOneChannel
.entry kernel_HistogramRectOneChannel(
	.param .u64 .ptr .global .align 8 kernel_HistogramRectOneChannel_param_0,
	.param .u32 kernel_HistogramRectOneChannel_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectOneChannel_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<37>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd5, [kernel_HistogramRectOneChannel_param_0];
	ld.param.u32 	%r5, [kernel_HistogramRectOneChannel_param_1];
	ld.param.u64 	%rd6, [kernel_HistogramRectOneChannel_param_2];
	mov.b32	%r6, %envreg3;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r1, %ntid.x;
	mad.lo.s32 	%r8, %r7, %r1, %r6;
	mov.u32 	%r9, %tid.x;
	add.s32 	%r2, %r8, %r9;
	shr.u32 	%r3, %r5, 3;
	setp.ge.u32	%p1, %r2, %r3;
	@%p1 bra 	BB14_3;

	and.b32  	%r4, %r2, 255;
	cvt.s64.s32	%rd26, %r2;
	mov.b32	%r10, %envreg6;
	mul.lo.s32 	%r11, %r1, %r10;
	cvt.s64.s32	%rd2, %r11;

BB14_2:
	and.b64  	%rd7, %rd26, 4294967295;
	shl.b64 	%rd8, %rd7, 3;
	add.s64 	%rd9, %rd5, %rd8;
	ld.global.v4.u8 	{%rs1, %rs2, %rs3, %rs4}, [%rd9];
	mul.wide.u16 	%r12, %rs1, 256;
	add.s32 	%r13, %r12, %r4;
	mul.wide.s32 	%rd10, %r13, 4;
	add.s64 	%rd11, %rd6, %rd10;
	ld.global.v4.u8 	{%rs5, %rs6, %rs7, %rs8}, [%rd9+4];
	atom.global.add.u32 	%r14, [%rd11], 1;
	mul.wide.u16 	%r15, %rs2, 256;
	add.s32 	%r16, %r15, %r4;
	mul.wide.s32 	%rd12, %r16, 4;
	add.s64 	%rd13, %rd6, %rd12;
	atom.global.add.u32 	%r17, [%rd13], 1;
	mul.wide.u16 	%r18, %rs3, 256;
	add.s32 	%r19, %r18, %r4;
	mul.wide.s32 	%rd14, %r19, 4;
	add.s64 	%rd15, %rd6, %rd14;
	atom.global.add.u32 	%r20, [%rd15], 1;
	mul.wide.u16 	%r21, %rs4, 256;
	add.s32 	%r22, %r21, %r4;
	mul.wide.s32 	%rd16, %r22, 4;
	add.s64 	%rd17, %rd6, %rd16;
	atom.global.add.u32 	%r23, [%rd17], 1;
	mul.wide.u16 	%r24, %rs5, 256;
	add.s32 	%r25, %r24, %r4;
	mul.wide.s32 	%rd18, %r25, 4;
	add.s64 	%rd19, %rd6, %rd18;
	atom.global.add.u32 	%r26, [%rd19], 1;
	mul.wide.u16 	%r27, %rs6, 256;
	add.s32 	%r28, %r27, %r4;
	mul.wide.s32 	%rd20, %r28, 4;
	add.s64 	%rd21, %rd6, %rd20;
	atom.global.add.u32 	%r29, [%rd21], 1;
	mul.wide.u16 	%r30, %rs7, 256;
	add.s32 	%r31, %r30, %r4;
	mul.wide.s32 	%rd22, %r31, 4;
	add.s64 	%rd23, %rd6, %rd22;
	atom.global.add.u32 	%r32, [%rd23], 1;
	mul.wide.u16 	%r33, %rs8, 256;
	add.s32 	%r34, %r33, %r4;
	mul.wide.s32 	%rd24, %r34, 4;
	add.s64 	%rd25, %rd6, %rd24;
	atom.global.add.u32 	%r35, [%rd25], 1;
	add.s64 	%rd26, %rd2, %rd7;
	cvt.u32.u64	%r36, %rd26;
	setp.lt.u32	%p2, %r36, %r3;
	@%p2 bra 	BB14_2;

BB14_3:
	ret;
}

	// .globl	kernel_HistogramRectAllChannelsReduction
.entry kernel_HistogramRectAllChannelsReduction(
	.param .u32 kernel_HistogramRectAllChannelsReduction_param_0,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectAllChannelsReduction_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectAllChannelsReduction_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<67>;
	.reg .b64 	%rd<11>;
	// demoted variable
	.shared .align 4 .b8 kernel_HistogramRectAllChannelsReduction$localHist[1024];

	ld.param.u64 	%rd3, [kernel_HistogramRectAllChannelsReduction_param_1];
	ld.param.u64 	%rd4, [kernel_HistogramRectAllChannelsReduction_param_2];
	mov.b32	%r1, %envreg0;
	mov.u32 	%r2, %ctaid.x;
	add.s32 	%r3, %r2, %r1;
	cvt.s64.s32	%rd1, %r3;
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r58, 0;
	setp.gt.u32	%p2, %r4, 255;
	@%p2 bra 	BB15_3;

	cvt.u32.u64	%r32, %rd1;
	and.b32  	%r33, %r32, 16776960;
	add.s32 	%r56, %r4, -256;
	mad.lo.s32 	%r34, %r33, 256, %r4;
	and.b32  	%r36, %r3, 255;
	mad.lo.s32 	%r55, %r36, 256, %r34;
	mov.u32 	%r58, 0;

BB15_2:
	mul.wide.u32 	%rd5, %r55, 4;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.u32 	%r37, [%rd6];
	add.s32 	%r58, %r37, %r58;
	add.s32 	%r55, %r55, 256;
	add.s32 	%r56, %r56, 256;
	setp.gt.u32	%p3, %r56, -257;
	@%p3 bra 	BB15_2;

BB15_3:
	mul.wide.s32 	%rd7, %r4, 4;
	mov.u64 	%rd8, kernel_HistogramRectAllChannelsReduction$localHist;
	add.s64 	%rd2, %rd8, %rd7;
	st.shared.u32 	[%rd2], %r58;
	bar.sync 	0;
	setp.gt.u32	%p4, %r4, 127;
	@%p4 bra 	BB15_5;

	ld.shared.u32 	%r58, [%rd2+512];

BB15_5:
	bar.sync 	0;
	@%p4 bra 	BB15_7;

	ld.shared.u32 	%r38, [%rd2];
	add.s32 	%r39, %r38, %r58;
	st.shared.u32 	[%rd2], %r39;

BB15_7:
	bar.sync 	0;
	setp.gt.u32	%p6, %r4, 63;
	@%p6 bra 	BB15_9;

	ld.shared.u32 	%r58, [%rd2+256];

BB15_9:
	bar.sync 	0;
	@%p6 bra 	BB15_11;

	ld.shared.u32 	%r40, [%rd2];
	add.s32 	%r41, %r40, %r58;
	st.shared.u32 	[%rd2], %r41;

BB15_11:
	bar.sync 	0;
	setp.gt.u32	%p8, %r4, 31;
	@%p8 bra 	BB15_13;

	ld.shared.u32 	%r58, [%rd2+128];

BB15_13:
	bar.sync 	0;
	@%p8 bra 	BB15_15;

	ld.shared.u32 	%r42, [%rd2];
	add.s32 	%r43, %r42, %r58;
	st.shared.u32 	[%rd2], %r43;

BB15_15:
	bar.sync 	0;
	setp.gt.u32	%p10, %r4, 15;
	@%p10 bra 	BB15_17;

	ld.shared.u32 	%r58, [%rd2+64];

BB15_17:
	bar.sync 	0;
	@%p10 bra 	BB15_19;

	ld.shared.u32 	%r44, [%rd2];
	add.s32 	%r45, %r44, %r58;
	st.shared.u32 	[%rd2], %r45;

BB15_19:
	bar.sync 	0;
	setp.gt.u32	%p12, %r4, 7;
	@%p12 bra 	BB15_21;

	ld.shared.u32 	%r58, [%rd2+32];

BB15_21:
	bar.sync 	0;
	@%p12 bra 	BB15_23;

	ld.shared.u32 	%r46, [%rd2];
	add.s32 	%r47, %r46, %r58;
	st.shared.u32 	[%rd2], %r47;

BB15_23:
	bar.sync 	0;
	setp.gt.u32	%p14, %r4, 3;
	@%p14 bra 	BB15_25;

	ld.shared.u32 	%r58, [%rd2+16];

BB15_25:
	bar.sync 	0;
	@%p14 bra 	BB15_27;

	ld.shared.u32 	%r48, [%rd2];
	add.s32 	%r49, %r48, %r58;
	st.shared.u32 	[%rd2], %r49;

BB15_27:
	bar.sync 	0;
	setp.gt.u32	%p16, %r4, 1;
	@%p16 bra 	BB15_29;

	ld.shared.u32 	%r58, [%rd2+8];

BB15_29:
	bar.sync 	0;
	@%p16 bra 	BB15_31;

	ld.shared.u32 	%r50, [%rd2];
	add.s32 	%r51, %r50, %r58;
	st.shared.u32 	[%rd2], %r51;

BB15_31:
	bar.sync 	0;
	setp.ne.s32	%p18, %r4, 0;
	@%p18 bra 	BB15_33;

	ld.shared.u32 	%r58, [%rd2+4];

BB15_33:
	bar.sync 	0;
	@%p18 bra 	BB15_35;

	ld.shared.u32 	%r52, [%rd2];
	add.s32 	%r53, %r52, %r58;
	st.shared.u32 	[%rd2], %r53;

BB15_35:
	setp.eq.s32	%p1, %r4, 0;
	bar.sync 	0;
	@!%p1 bra 	BB15_37;
	bra.uni 	BB15_36;

BB15_36:
	ld.shared.u32 	%r54, [kernel_HistogramRectAllChannelsReduction$localHist];
	mul.wide.s32 	%rd9, %r3, 4;
	add.s64 	%rd10, %rd4, %rd9;
	st.global.u32 	[%rd10], %r54;

BB15_37:
	ret;
}

	// .globl	kernel_HistogramRectOneChannelReduction
.entry kernel_HistogramRectOneChannelReduction(
	.param .u32 kernel_HistogramRectOneChannelReduction_param_0,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectOneChannelReduction_param_1,
	.param .u64 .ptr .global .align 4 kernel_HistogramRectOneChannelReduction_param_2
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<101>;
	.reg .b64 	%rd<20>;
	// demoted variable
	.shared .align 4 .b8 kernel_HistogramRectOneChannelReduction$localHist[1024];

	ld.param.u64 	%rd6, [kernel_HistogramRectOneChannelReduction_param_1];
	ld.param.u64 	%rd7, [kernel_HistogramRectOneChannelReduction_param_2];
	mov.b32	%r1, %envreg0;
	mov.u32 	%r2, %ctaid.x;
	add.s32 	%r3, %r2, %r1;
	cvt.s64.s32	%rd1, %r3;
	mov.u32 	%r4, %tid.x;
	mov.u32 	%r92, 0;
	setp.gt.s32	%p2, %r4, 255;
	@%p2 bra 	BB16_10;

	cvt.u32.u64	%r44, %rd1;
	and.b32  	%r45, %r44, 255;
	shl.b32 	%r5, %r45, 8;
	add.s32 	%r46, %r4, 255;
	setp.gt.s32	%p3, %r4, 0;
	selp.b32	%r47, %r46, 255, %p3;
	sub.s32 	%r48, %r47, %r4;
	shr.u32 	%r49, %r48, 8;
	add.s32 	%r6, %r49, 1;
	and.b32  	%r7, %r6, 3;
	setp.eq.s32	%p4, %r7, 0;
	mov.u32 	%r92, 0;
	mov.u32 	%r88, %r4;
	@%p4 bra 	BB16_7;

	setp.eq.s32	%p5, %r7, 1;
	mov.u32 	%r87, 0;
	mov.u32 	%r86, %r4;
	@%p5 bra 	BB16_6;

	setp.eq.s32	%p6, %r7, 2;
	mov.u32 	%r85, 0;
	mov.u32 	%r84, %r4;
	@%p6 bra 	BB16_5;

	add.s32 	%r52, %r4, %r5;
	mul.wide.s32 	%rd8, %r52, 4;
	add.s64 	%rd9, %rd6, %rd8;
	ld.global.u32 	%r85, [%rd9];
	add.s32 	%r84, %r4, 256;

BB16_5:
	add.s32 	%r53, %r84, %r5;
	mul.wide.s32 	%rd10, %r53, 4;
	add.s64 	%rd11, %rd6, %rd10;
	ld.global.u32 	%r54, [%rd11];
	add.s32 	%r87, %r54, %r85;
	add.s32 	%r86, %r84, 256;

BB16_6:
	add.s32 	%r55, %r86, %r5;
	mul.wide.s32 	%rd12, %r55, 4;
	add.s64 	%rd13, %rd6, %rd12;
	ld.global.u32 	%r56, [%rd13];
	add.s32 	%r92, %r56, %r87;
	add.s32 	%r88, %r86, 256;

BB16_7:
	setp.lt.u32	%p7, %r6, 4;
	@%p7 bra 	BB16_10;

	add.s32 	%r90, %r88, -256;
	and.b32  	%r58, %r3, 255;
	mad.lo.s32 	%r59, %r58, 256, %r88;
	mul.wide.s32 	%rd14, %r59, 4;
	add.s64 	%rd19, %rd6, %rd14;

BB16_9:
	ld.global.u32 	%r60, [%rd19];
	add.s32 	%r61, %r60, %r92;
	ld.global.u32 	%r62, [%rd19+1024];
	add.s32 	%r63, %r62, %r61;
	ld.global.u32 	%r64, [%rd19+2048];
	add.s32 	%r65, %r64, %r63;
	ld.global.u32 	%r66, [%rd19+3072];
	add.s32 	%r92, %r66, %r65;
	add.s64 	%rd19, %rd19, 4096;
	add.s32 	%r90, %r90, 1024;
	setp.lt.s32	%p8, %r90, 0;
	@%p8 bra 	BB16_9;

BB16_10:
	mul.wide.s32 	%rd15, %r4, 4;
	mov.u64 	%rd16, kernel_HistogramRectOneChannelReduction$localHist;
	add.s64 	%rd5, %rd16, %rd15;
	st.shared.u32 	[%rd5], %r92;
	bar.sync 	0;
	setp.gt.u32	%p9, %r4, 127;
	@%p9 bra 	BB16_12;

	ld.shared.u32 	%r92, [%rd5+512];

BB16_12:
	bar.sync 	0;
	@%p9 bra 	BB16_14;

	ld.shared.u32 	%r67, [%rd5];
	add.s32 	%r68, %r67, %r92;
	st.shared.u32 	[%rd5], %r68;

BB16_14:
	bar.sync 	0;
	setp.gt.u32	%p11, %r4, 63;
	@%p11 bra 	BB16_16;

	ld.shared.u32 	%r92, [%rd5+256];

BB16_16:
	bar.sync 	0;
	@%p11 bra 	BB16_18;

	ld.shared.u32 	%r69, [%rd5];
	add.s32 	%r70, %r69, %r92;
	st.shared.u32 	[%rd5], %r70;

BB16_18:
	bar.sync 	0;
	setp.gt.u32	%p13, %r4, 31;
	@%p13 bra 	BB16_20;

	ld.shared.u32 	%r92, [%rd5+128];

BB16_20:
	bar.sync 	0;
	@%p13 bra 	BB16_22;

	ld.shared.u32 	%r71, [%rd5];
	add.s32 	%r72, %r71, %r92;
	st.shared.u32 	[%rd5], %r72;

BB16_22:
	bar.sync 	0;
	setp.gt.u32	%p15, %r4, 15;
	@%p15 bra 	BB16_24;

	ld.shared.u32 	%r92, [%rd5+64];

BB16_24:
	bar.sync 	0;
	@%p15 bra 	BB16_26;

	ld.shared.u32 	%r73, [%rd5];
	add.s32 	%r74, %r73, %r92;
	st.shared.u32 	[%rd5], %r74;

BB16_26:
	bar.sync 	0;
	setp.gt.u32	%p17, %r4, 7;
	@%p17 bra 	BB16_28;

	ld.shared.u32 	%r92, [%rd5+32];

BB16_28:
	bar.sync 	0;
	@%p17 bra 	BB16_30;

	ld.shared.u32 	%r75, [%rd5];
	add.s32 	%r76, %r75, %r92;
	st.shared.u32 	[%rd5], %r76;

BB16_30:
	bar.sync 	0;
	setp.gt.u32	%p19, %r4, 3;
	@%p19 bra 	BB16_32;

	ld.shared.u32 	%r92, [%rd5+16];

BB16_32:
	bar.sync 	0;
	@%p19 bra 	BB16_34;

	ld.shared.u32 	%r77, [%rd5];
	add.s32 	%r78, %r77, %r92;
	st.shared.u32 	[%rd5], %r78;

BB16_34:
	bar.sync 	0;
	setp.gt.u32	%p21, %r4, 1;
	@%p21 bra 	BB16_36;

	ld.shared.u32 	%r92, [%rd5+8];

BB16_36:
	bar.sync 	0;
	@%p21 bra 	BB16_38;

	ld.shared.u32 	%r79, [%rd5];
	add.s32 	%r80, %r79, %r92;
	st.shared.u32 	[%rd5], %r80;

BB16_38:
	bar.sync 	0;
	setp.ne.s32	%p23, %r4, 0;
	@%p23 bra 	BB16_40;

	ld.shared.u32 	%r92, [%rd5+4];

BB16_40:
	bar.sync 	0;
	@%p23 bra 	BB16_42;

	ld.shared.u32 	%r81, [%rd5];
	add.s32 	%r82, %r81, %r92;
	st.shared.u32 	[%rd5], %r82;

BB16_42:
	setp.eq.s32	%p1, %r4, 0;
	bar.sync 	0;
	@!%p1 bra 	BB16_44;
	bra.uni 	BB16_43;

BB16_43:
	ld.shared.u32 	%r83, [kernel_HistogramRectOneChannelReduction$localHist];
	mul.wide.s32 	%rd17, %r3, 4;
	add.s64 	%rd18, %rd7, %rd17;
	st.global.u32 	[%rd18], %r83;

BB16_44:
	ret;
}

	// .globl	kernel_ThresholdRectToPix
.entry kernel_ThresholdRectToPix(
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_0,
	.param .u32 kernel_ThresholdRectToPix_param_1,
	.param .u32 kernel_ThresholdRectToPix_param_2,
	.param .u32 kernel_ThresholdRectToPix_param_3,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_4,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_5,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_param_6
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<132>;
	.reg .b16 	%rs<65>;
	.reg .b32 	%r<279>;
	.reg .b64 	%rd<17>;


	ld.param.u32 	%r95, [kernel_ThresholdRectToPix_param_1];
	ld.param.u32 	%r94, [kernel_ThresholdRectToPix_param_3];
	ld.param.u64 	%rd7, [kernel_ThresholdRectToPix_param_4];
	ld.param.u64 	%rd8, [kernel_ThresholdRectToPix_param_5];
	ld.global.u32 	%r1, [%rd7];
	ld.global.u32 	%r2, [%rd8];
	ld.global.u32 	%r3, [%rd7+4];
	ld.global.u32 	%r4, [%rd8+4];
	ld.global.u32 	%r5, [%rd7+8];
	ld.global.u32 	%r6, [%rd8+8];
	ld.global.u32 	%r7, [%rd7+12];
	ld.global.u32 	%r8, [%rd8+12];
	mov.u32 	%r96, %ctaid.x;
	mov.u32 	%r9, %ntid.x;
	mov.b32	%r97, %envreg3;
	mad.lo.s32 	%r98, %r96, %r9, %r97;
	mov.u32 	%r99, %tid.x;
	add.s32 	%r243, %r98, %r99;
	mul.lo.s32 	%r11, %r94, %r95;
	setp.ge.u32	%p1, %r243, %r11;
	@%p1 bra 	BB17_69;

	cvt.s64.s32	%rd16, %r243;
	mov.b32	%r100, %envreg6;
	mul.lo.s32 	%r101, %r9, %r100;
	cvt.s64.s32	%rd2, %r101;

BB17_2:
	ld.param.u32 	%r229, [kernel_ThresholdRectToPix_param_2];
	ld.param.u32 	%r228, [kernel_ThresholdRectToPix_param_3];
	mov.u32 	%r244, 0;
	mov.u32 	%r245, %r244;
	mov.u32 	%r246, %r244;

BB17_3:
	rem.u32 	%r234, %r243, %r228;
	shl.b32 	%r233, %r234, 5;
	div.u32 	%r231, %r243, %r228;
	mul.lo.s32 	%r230, %r231, %r229;
	ld.param.u64 	%rd15, [kernel_ThresholdRectToPix_param_0];
	shl.b32 	%r107, %r245, 3;
	add.s32 	%r108, %r230, %r107;
	add.s32 	%r109, %r108, %r233;
	mul.wide.s32 	%rd9, %r109, 4;
	add.s64 	%rd10, %rd15, %rd9;
	ld.global.v4.u8 	{%rs33, %rs34, %rs35, %rs36}, [%rd10];
	ld.global.v4.u8 	{%rs37, %rs38, %rs39, %rs40}, [%rd10+4];
	ld.global.v4.u8 	{%rs41, %rs42, %rs43, %rs44}, [%rd10+8];
	ld.global.v4.u8 	{%rs45, %rs46, %rs47, %rs48}, [%rd10+12];
	ld.global.v4.u8 	{%rs49, %rs50, %rs51, %rs52}, [%rd10+16];
	ld.global.v4.u8 	{%rs53, %rs54, %rs55, %rs56}, [%rd10+20];
	ld.global.v4.u8 	{%rs57, %rs58, %rs59, %rs60}, [%rd10+24];
	ld.global.v4.u8 	{%rs61, %rs62, %rs63, %rs64}, [%rd10+28];
	setp.lt.s32	%p2, %r2, 0;
	@%p2 bra 	BB17_5;

	mov.u32 	%r242, -2147483648;
	shr.u32 	%r241, %r242, %r244;
	setp.eq.s32	%p3, %r2, 0;
	cvt.u32.u16	%r111, %rs33;
	and.b32  	%r112, %r111, 255;
	setp.gt.s32	%p4, %r112, %r1;
	xor.pred  	%p5, %p4, %p3;
	selp.b32	%r113, 0, %r241, %p5;
	or.b32  	%r246, %r246, %r113;

BB17_5:
	setp.lt.s32	%p6, %r4, 0;
	@%p6 bra 	BB17_7;

	mov.u32 	%r240, -2147483648;
	shr.u32 	%r239, %r240, %r244;
	setp.eq.s32	%p7, %r4, 0;
	cvt.u32.u16	%r114, %rs34;
	and.b32  	%r115, %r114, 255;
	setp.gt.s32	%p8, %r115, %r3;
	xor.pred  	%p9, %p8, %p7;
	selp.b32	%r116, 0, %r239, %p9;
	or.b32  	%r246, %r246, %r116;

BB17_7:
	setp.lt.s32	%p10, %r6, 0;
	@%p10 bra 	BB17_9;

	mov.u32 	%r238, -2147483648;
	shr.u32 	%r237, %r238, %r244;
	setp.eq.s32	%p11, %r6, 0;
	cvt.u32.u16	%r117, %rs35;
	and.b32  	%r118, %r117, 255;
	setp.gt.s32	%p12, %r118, %r5;
	xor.pred  	%p13, %p12, %p11;
	selp.b32	%r119, 0, %r237, %p13;
	or.b32  	%r246, %r246, %r119;

BB17_9:
	setp.lt.s32	%p14, %r8, 0;
	@%p14 bra 	BB17_11;

	mov.u32 	%r236, -2147483648;
	shr.u32 	%r235, %r236, %r244;
	setp.eq.s32	%p15, %r8, 0;
	cvt.u32.u16	%r120, %rs36;
	and.b32  	%r121, %r120, 255;
	setp.gt.s32	%p16, %r121, %r7;
	xor.pred  	%p17, %p16, %p15;
	selp.b32	%r122, 0, %r235, %p17;
	or.b32  	%r246, %r246, %r122;

BB17_11:
	mov.u32 	%r221, -2147483648;
	add.s32 	%r123, %r244, 1;
	shr.u32 	%r27, %r221, %r123;
	@%p2 bra 	BB17_13;

	setp.eq.s32	%p19, %r2, 0;
	cvt.u32.u16	%r125, %rs37;
	and.b32  	%r126, %r125, 255;
	setp.gt.s32	%p20, %r126, %r1;
	xor.pred  	%p21, %p20, %p19;
	selp.b32	%r127, 0, %r27, %p21;
	or.b32  	%r246, %r246, %r127;

BB17_13:
	@%p6 bra 	BB17_15;

	setp.eq.s32	%p23, %r4, 0;
	cvt.u32.u16	%r128, %rs38;
	and.b32  	%r129, %r128, 255;
	setp.gt.s32	%p24, %r129, %r3;
	xor.pred  	%p25, %p24, %p23;
	selp.b32	%r130, 0, %r27, %p25;
	or.b32  	%r246, %r246, %r130;

BB17_15:
	@%p10 bra 	BB17_17;

	setp.eq.s32	%p27, %r6, 0;
	cvt.u32.u16	%r131, %rs39;
	and.b32  	%r132, %r131, 255;
	setp.gt.s32	%p28, %r132, %r5;
	xor.pred  	%p29, %p28, %p27;
	selp.b32	%r133, 0, %r27, %p29;
	or.b32  	%r246, %r246, %r133;

BB17_17:
	@%p14 bra 	BB17_19;

	setp.eq.s32	%p31, %r8, 0;
	cvt.u32.u16	%r134, %rs40;
	and.b32  	%r135, %r134, 255;
	setp.gt.s32	%p32, %r135, %r7;
	xor.pred  	%p33, %p32, %p31;
	selp.b32	%r136, 0, %r27, %p33;
	or.b32  	%r246, %r246, %r136;

BB17_19:
	mov.u32 	%r222, -2147483648;
	add.s32 	%r137, %r244, 2;
	shr.u32 	%r36, %r222, %r137;
	@%p2 bra 	BB17_21;

	setp.eq.s32	%p35, %r2, 0;
	cvt.u32.u16	%r139, %rs41;
	and.b32  	%r140, %r139, 255;
	setp.gt.s32	%p36, %r140, %r1;
	xor.pred  	%p37, %p36, %p35;
	selp.b32	%r141, 0, %r36, %p37;
	or.b32  	%r246, %r246, %r141;

BB17_21:
	@%p6 bra 	BB17_23;

	setp.eq.s32	%p39, %r4, 0;
	cvt.u32.u16	%r142, %rs42;
	and.b32  	%r143, %r142, 255;
	setp.gt.s32	%p40, %r143, %r3;
	xor.pred  	%p41, %p40, %p39;
	selp.b32	%r144, 0, %r36, %p41;
	or.b32  	%r246, %r246, %r144;

BB17_23:
	@%p10 bra 	BB17_25;

	setp.eq.s32	%p43, %r6, 0;
	cvt.u32.u16	%r145, %rs43;
	and.b32  	%r146, %r145, 255;
	setp.gt.s32	%p44, %r146, %r5;
	xor.pred  	%p45, %p44, %p43;
	selp.b32	%r147, 0, %r36, %p45;
	or.b32  	%r246, %r246, %r147;

BB17_25:
	@%p14 bra 	BB17_27;

	setp.eq.s32	%p47, %r8, 0;
	cvt.u32.u16	%r148, %rs44;
	and.b32  	%r149, %r148, 255;
	setp.gt.s32	%p48, %r149, %r7;
	xor.pred  	%p49, %p48, %p47;
	selp.b32	%r150, 0, %r36, %p49;
	or.b32  	%r246, %r246, %r150;

BB17_27:
	mov.u32 	%r223, -2147483648;
	add.s32 	%r151, %r244, 3;
	shr.u32 	%r45, %r223, %r151;
	@%p2 bra 	BB17_29;

	setp.eq.s32	%p51, %r2, 0;
	cvt.u32.u16	%r153, %rs45;
	and.b32  	%r154, %r153, 255;
	setp.gt.s32	%p52, %r154, %r1;
	xor.pred  	%p53, %p52, %p51;
	selp.b32	%r155, 0, %r45, %p53;
	or.b32  	%r246, %r246, %r155;

BB17_29:
	@%p6 bra 	BB17_31;

	setp.eq.s32	%p55, %r4, 0;
	cvt.u32.u16	%r156, %rs46;
	and.b32  	%r157, %r156, 255;
	setp.gt.s32	%p56, %r157, %r3;
	xor.pred  	%p57, %p56, %p55;
	selp.b32	%r158, 0, %r45, %p57;
	or.b32  	%r246, %r246, %r158;

BB17_31:
	@%p10 bra 	BB17_33;

	setp.eq.s32	%p59, %r6, 0;
	cvt.u32.u16	%r159, %rs47;
	and.b32  	%r160, %r159, 255;
	setp.gt.s32	%p60, %r160, %r5;
	xor.pred  	%p61, %p60, %p59;
	selp.b32	%r161, 0, %r45, %p61;
	or.b32  	%r246, %r246, %r161;

BB17_33:
	@%p14 bra 	BB17_35;

	setp.eq.s32	%p63, %r8, 0;
	cvt.u32.u16	%r162, %rs48;
	and.b32  	%r163, %r162, 255;
	setp.gt.s32	%p64, %r163, %r7;
	xor.pred  	%p65, %p64, %p63;
	selp.b32	%r164, 0, %r45, %p65;
	or.b32  	%r246, %r246, %r164;

BB17_35:
	mov.u32 	%r224, -2147483648;
	add.s32 	%r165, %r244, 4;
	shr.u32 	%r54, %r224, %r165;
	@%p2 bra 	BB17_37;

	setp.eq.s32	%p67, %r2, 0;
	cvt.u32.u16	%r167, %rs49;
	and.b32  	%r168, %r167, 255;
	setp.gt.s32	%p68, %r168, %r1;
	xor.pred  	%p69, %p68, %p67;
	selp.b32	%r169, 0, %r54, %p69;
	or.b32  	%r246, %r246, %r169;

BB17_37:
	@%p6 bra 	BB17_39;

	setp.eq.s32	%p71, %r4, 0;
	cvt.u32.u16	%r170, %rs50;
	and.b32  	%r171, %r170, 255;
	setp.gt.s32	%p72, %r171, %r3;
	xor.pred  	%p73, %p72, %p71;
	selp.b32	%r172, 0, %r54, %p73;
	or.b32  	%r246, %r246, %r172;

BB17_39:
	@%p10 bra 	BB17_41;

	setp.eq.s32	%p75, %r6, 0;
	cvt.u32.u16	%r173, %rs51;
	and.b32  	%r174, %r173, 255;
	setp.gt.s32	%p76, %r174, %r5;
	xor.pred  	%p77, %p76, %p75;
	selp.b32	%r175, 0, %r54, %p77;
	or.b32  	%r246, %r246, %r175;

BB17_41:
	@%p14 bra 	BB17_43;

	setp.eq.s32	%p79, %r8, 0;
	cvt.u32.u16	%r176, %rs52;
	and.b32  	%r177, %r176, 255;
	setp.gt.s32	%p80, %r177, %r7;
	xor.pred  	%p81, %p80, %p79;
	selp.b32	%r178, 0, %r54, %p81;
	or.b32  	%r246, %r246, %r178;

BB17_43:
	mov.u32 	%r225, -2147483648;
	add.s32 	%r179, %r244, 5;
	shr.u32 	%r63, %r225, %r179;
	@%p2 bra 	BB17_45;

	setp.eq.s32	%p83, %r2, 0;
	cvt.u32.u16	%r181, %rs53;
	and.b32  	%r182, %r181, 255;
	setp.gt.s32	%p84, %r182, %r1;
	xor.pred  	%p85, %p84, %p83;
	selp.b32	%r183, 0, %r63, %p85;
	or.b32  	%r246, %r246, %r183;

BB17_45:
	@%p6 bra 	BB17_47;

	setp.eq.s32	%p87, %r4, 0;
	cvt.u32.u16	%r184, %rs54;
	and.b32  	%r185, %r184, 255;
	setp.gt.s32	%p88, %r185, %r3;
	xor.pred  	%p89, %p88, %p87;
	selp.b32	%r186, 0, %r63, %p89;
	or.b32  	%r246, %r246, %r186;

BB17_47:
	@%p10 bra 	BB17_49;

	setp.eq.s32	%p91, %r6, 0;
	cvt.u32.u16	%r187, %rs55;
	and.b32  	%r188, %r187, 255;
	setp.gt.s32	%p92, %r188, %r5;
	xor.pred  	%p93, %p92, %p91;
	selp.b32	%r189, 0, %r63, %p93;
	or.b32  	%r246, %r246, %r189;

BB17_49:
	@%p14 bra 	BB17_51;

	setp.eq.s32	%p95, %r8, 0;
	cvt.u32.u16	%r190, %rs56;
	and.b32  	%r191, %r190, 255;
	setp.gt.s32	%p96, %r191, %r7;
	xor.pred  	%p97, %p96, %p95;
	selp.b32	%r192, 0, %r63, %p97;
	or.b32  	%r246, %r246, %r192;

BB17_51:
	mov.u32 	%r226, -2147483648;
	add.s32 	%r193, %r244, 6;
	shr.u32 	%r72, %r226, %r193;
	@%p2 bra 	BB17_53;

	setp.eq.s32	%p99, %r2, 0;
	cvt.u32.u16	%r195, %rs57;
	and.b32  	%r196, %r195, 255;
	setp.gt.s32	%p100, %r196, %r1;
	xor.pred  	%p101, %p100, %p99;
	selp.b32	%r197, 0, %r72, %p101;
	or.b32  	%r246, %r246, %r197;

BB17_53:
	@%p6 bra 	BB17_55;

	setp.eq.s32	%p103, %r4, 0;
	cvt.u32.u16	%r198, %rs58;
	and.b32  	%r199, %r198, 255;
	setp.gt.s32	%p104, %r199, %r3;
	xor.pred  	%p105, %p104, %p103;
	selp.b32	%r200, 0, %r72, %p105;
	or.b32  	%r246, %r246, %r200;

BB17_55:
	@%p10 bra 	BB17_57;

	setp.eq.s32	%p107, %r6, 0;
	cvt.u32.u16	%r201, %rs59;
	and.b32  	%r202, %r201, 255;
	setp.gt.s32	%p108, %r202, %r5;
	xor.pred  	%p109, %p108, %p107;
	selp.b32	%r203, 0, %r72, %p109;
	or.b32  	%r246, %r246, %r203;

BB17_57:
	@%p14 bra 	BB17_59;

	setp.eq.s32	%p111, %r8, 0;
	cvt.u32.u16	%r204, %rs60;
	and.b32  	%r205, %r204, 255;
	setp.gt.s32	%p112, %r205, %r7;
	xor.pred  	%p113, %p112, %p111;
	selp.b32	%r206, 0, %r72, %p113;
	or.b32  	%r246, %r246, %r206;

BB17_59:
	mov.u32 	%r227, -2147483648;
	add.s32 	%r207, %r244, 7;
	shr.u32 	%r81, %r227, %r207;
	@%p2 bra 	BB17_61;

	setp.eq.s32	%p115, %r2, 0;
	cvt.u32.u16	%r209, %rs61;
	and.b32  	%r210, %r209, 255;
	setp.gt.s32	%p116, %r210, %r1;
	xor.pred  	%p117, %p116, %p115;
	selp.b32	%r211, 0, %r81, %p117;
	or.b32  	%r246, %r246, %r211;

BB17_61:
	@%p6 bra 	BB17_63;

	setp.eq.s32	%p119, %r4, 0;
	cvt.u32.u16	%r212, %rs62;
	and.b32  	%r213, %r212, 255;
	setp.gt.s32	%p120, %r213, %r3;
	xor.pred  	%p121, %p120, %p119;
	selp.b32	%r214, 0, %r81, %p121;
	or.b32  	%r246, %r246, %r214;

BB17_63:
	@%p10 bra 	BB17_65;

	setp.eq.s32	%p123, %r6, 0;
	cvt.u32.u16	%r215, %rs63;
	and.b32  	%r216, %r215, 255;
	setp.gt.s32	%p124, %r216, %r5;
	xor.pred  	%p125, %p124, %p123;
	selp.b32	%r217, 0, %r81, %p125;
	or.b32  	%r246, %r246, %r217;

BB17_65:
	@%p14 bra 	BB17_67;

	setp.eq.s32	%p127, %r8, 0;
	cvt.u32.u16	%r218, %rs64;
	and.b32  	%r219, %r218, 255;
	setp.gt.s32	%p128, %r219, %r7;
	xor.pred  	%p129, %p128, %p127;
	selp.b32	%r220, 0, %r81, %p129;
	or.b32  	%r246, %r246, %r220;

BB17_67:
	add.s32 	%r245, %r245, 1;
	add.s32 	%r244, %r244, 8;
	setp.lt.s32	%p130, %r244, 32;
	@%p130 bra 	BB17_3;

	ld.param.u64 	%rd14, [kernel_ThresholdRectToPix_param_6];
	and.b64  	%rd11, %rd16, 4294967295;
	shl.b64 	%rd12, %rd11, 2;
	add.s64 	%rd13, %rd14, %rd12;
	st.global.u32 	[%rd13], %r246;
	add.s64 	%rd16, %rd2, %rd11;
	cvt.u32.u64	%r243, %rd16;
	setp.lt.u32	%p131, %r243, %r11;
	@%p131 bra 	BB17_2;

BB17_69:
	ret;
}

	// .globl	kernel_ThresholdRectToPix_OneChan
.entry kernel_ThresholdRectToPix_OneChan(
	.param .u64 .ptr .global .align 8 kernel_ThresholdRectToPix_OneChan_param_0,
	.param .u32 kernel_ThresholdRectToPix_OneChan_param_1,
	.param .u32 kernel_ThresholdRectToPix_OneChan_param_2,
	.param .u32 kernel_ThresholdRectToPix_OneChan_param_3,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_OneChan_param_4,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_OneChan_param_5,
	.param .u64 .ptr .global .align 4 kernel_ThresholdRectToPix_OneChan_param_6
)
.reqntid 256, 1, 1
{
	.reg .pred 	%p<36>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<94>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd5, [kernel_ThresholdRectToPix_OneChan_param_0];
	ld.param.u32 	%r29, [kernel_ThresholdRectToPix_OneChan_param_1];
	ld.param.u32 	%r30, [kernel_ThresholdRectToPix_OneChan_param_3];
	ld.param.u64 	%rd7, [kernel_ThresholdRectToPix_OneChan_param_4];
	ld.param.u64 	%rd8, [kernel_ThresholdRectToPix_OneChan_param_5];
	ld.param.u64 	%rd6, [kernel_ThresholdRectToPix_OneChan_param_6];
	ld.global.u32 	%r1, [%rd7];
	ld.global.u32 	%r2, [%rd8];
	mov.u32 	%r31, %ctaid.x;
	mov.u32 	%r3, %ntid.x;
	mov.b32	%r32, %envreg3;
	mad.lo.s32 	%r33, %r31, %r3, %r32;
	mov.u32 	%r34, %tid.x;
	add.s32 	%r82, %r33, %r34;
	mul.lo.s32 	%r35, %r30, %r29;
	setp.ge.u32	%p1, %r82, %r35;
	@%p1 bra 	BB18_29;

	cvt.s64.s32	%rd14, %r82;
	mov.b32	%r36, %envreg6;
	mul.lo.s32 	%r37, %r3, %r36;
	cvt.s64.s32	%rd2, %r37;

BB18_2:
	shl.b32 	%r6, %r82, 2;
	mov.u32 	%r83, 0;
	mov.u32 	%r84, %r83;
	mov.u32 	%r93, %r83;

BB18_3:
	add.s32 	%r41, %r6, %r84;
	mul.wide.u32 	%rd9, %r41, 8;
	add.s64 	%rd10, %rd5, %rd9;
	ld.global.v4.u8 	{%rs9, %rs10, %rs11, %rs12}, [%rd10+4];
	ld.global.v4.u8 	{%rs13, %rs14, %rs15, %rs16}, [%rd10];
	setp.lt.s32	%p2, %r2, 0;
	@%p2 bra 	BB18_6;

	setp.eq.s32	%p3, %r2, 0;
	cvt.u32.u16	%r42, %rs16;
	and.b32  	%r43, %r42, 255;
	setp.gt.s32	%p4, %r43, %r1;
	xor.pred  	%p5, %p4, %p3;
	@%p5 bra 	BB18_6;

	mov.u32 	%r44, -2147483648;
	shr.u32 	%r45, %r44, %r83;
	or.b32  	%r93, %r45, %r93;

BB18_6:
	@%p2 bra 	BB18_9;

	setp.eq.s32	%p7, %r2, 0;
	cvt.u32.u16	%r46, %rs15;
	and.b32  	%r47, %r46, 255;
	setp.gt.s32	%p8, %r47, %r1;
	xor.pred  	%p9, %p8, %p7;
	@%p9 bra 	BB18_9;

	add.s32 	%r48, %r83, 1;
	mov.u32 	%r49, -2147483648;
	shr.u32 	%r50, %r49, %r48;
	or.b32  	%r93, %r50, %r93;

BB18_9:
	@%p2 bra 	BB18_12;

	setp.eq.s32	%p11, %r2, 0;
	cvt.u32.u16	%r51, %rs14;
	and.b32  	%r52, %r51, 255;
	setp.gt.s32	%p12, %r52, %r1;
	xor.pred  	%p13, %p12, %p11;
	@%p13 bra 	BB18_12;

	add.s32 	%r53, %r83, 2;
	mov.u32 	%r54, -2147483648;
	shr.u32 	%r55, %r54, %r53;
	or.b32  	%r93, %r55, %r93;

BB18_12:
	@%p2 bra 	BB18_15;

	setp.eq.s32	%p15, %r2, 0;
	cvt.u32.u16	%r56, %rs13;
	and.b32  	%r57, %r56, 255;
	setp.gt.s32	%p16, %r57, %r1;
	xor.pred  	%p17, %p16, %p15;
	@%p17 bra 	BB18_15;

	add.s32 	%r58, %r83, 3;
	mov.u32 	%r59, -2147483648;
	shr.u32 	%r60, %r59, %r58;
	or.b32  	%r93, %r60, %r93;

BB18_15:
	@%p2 bra 	BB18_18;

	setp.eq.s32	%p19, %r2, 0;
	cvt.u32.u16	%r61, %rs12;
	and.b32  	%r62, %r61, 255;
	setp.gt.s32	%p20, %r62, %r1;
	xor.pred  	%p21, %p20, %p19;
	@%p21 bra 	BB18_18;

	add.s32 	%r63, %r83, 4;
	mov.u32 	%r64, -2147483648;
	shr.u32 	%r65, %r64, %r63;
	or.b32  	%r93, %r65, %r93;

BB18_18:
	@%p2 bra 	BB18_21;

	setp.eq.s32	%p23, %r2, 0;
	cvt.u32.u16	%r66, %rs11;
	and.b32  	%r67, %r66, 255;
	setp.gt.s32	%p24, %r67, %r1;
	xor.pred  	%p25, %p24, %p23;
	@%p25 bra 	BB18_21;

	add.s32 	%r68, %r83, 5;
	mov.u32 	%r69, -2147483648;
	shr.u32 	%r70, %r69, %r68;
	or.b32  	%r93, %r70, %r93;

BB18_21:
	@%p2 bra 	BB18_24;

	setp.eq.s32	%p27, %r2, 0;
	cvt.u32.u16	%r71, %rs10;
	and.b32  	%r72, %r71, 255;
	setp.gt.s32	%p28, %r72, %r1;
	xor.pred  	%p29, %p28, %p27;
	@%p29 bra 	BB18_24;

	add.s32 	%r73, %r83, 6;
	mov.u32 	%r74, -2147483648;
	shr.u32 	%r75, %r74, %r73;
	or.b32  	%r93, %r75, %r93;

BB18_24:
	@%p2 bra 	BB18_27;

	setp.eq.s32	%p31, %r2, 0;
	cvt.u32.u16	%r76, %rs9;
	and.b32  	%r77, %r76, 255;
	setp.gt.s32	%p32, %r77, %r1;
	xor.pred  	%p33, %p32, %p31;
	@%p33 bra 	BB18_27;

	add.s32 	%r78, %r83, 7;
	mov.u32 	%r79, -2147483648;
	shr.u32 	%r80, %r79, %r78;
	or.b32  	%r93, %r80, %r93;

BB18_27:
	add.s32 	%r84, %r84, 1;
	add.s32 	%r83, %r83, 8;
	setp.lt.s32	%p34, %r83, 32;
	@%p34 bra 	BB18_3;

	and.b64  	%rd11, %rd14, 4294967295;
	shl.b64 	%rd12, %rd11, 2;
	add.s64 	%rd13, %rd6, %rd12;
	st.global.u32 	[%rd13], %r93;
	add.s64 	%rd14, %rd2, %rd11;
	cvt.u32.u64	%r82, %rd14;
	setp.lt.u32	%p35, %r82, %r35;
	@%p35 bra 	BB18_2;

BB18_29:
	ret;
}


.metadata_section {

.metadata 0 {
	"cl_kernel_attributes",
	"kernel_HistogramRectAllChannels",
	"reqd_work_group_size(256,1,1)"
}

.metadata 1 {
	"cl_kernel_attributes",
	"kernel_HistogramRectOneChannel",
	"reqd_work_group_size(256,1,1)"
}

.metadata 2 {
	"cl_kernel_attributes",
	"kernel_HistogramRectAllChannelsReduction",
	"reqd_work_group_size(256,1,1)"
}

.metadata 3 {
	"cl_kernel_attributes",
	"kernel_HistogramRectOneChannelReduction",
	"reqd_work_group_size(256,1,1)"
}

.metadata 4 {
	"cl_kernel_attributes",
	"kernel_ThresholdRectToPix",
	"reqd_work_group_size(256,1,1)"
}

.metadata 5 {
	"cl_kernel_attributes",
	"kernel_ThresholdRectToPix_OneChan",
	"reqd_work_group_size(256,1,1)"
}

} // end of .metadata_section
  